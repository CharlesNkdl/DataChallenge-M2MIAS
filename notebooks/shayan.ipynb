{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-06-02T13:49:24.360195Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SimpleAdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    🎯 Ingénieur de Caractéristiques Simple mais Efficace\n",
    "    \n",
    "    Utilise exactement les mêmes techniques qui ont donné F1=0.9442\n",
    "    mais de manière optimisée\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transformation simple mais efficace\"\"\"\n",
    "        print(\"🔧 Feature Engineering Simple et Efficace...\")\n",
    "        \n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        \n",
    "        # 1. Statistiques de base importantes (comme dans le modèle original)\n",
    "        basic_stats = self._extract_basic_stats(X)\n",
    "        \n",
    "        # 2. Caractéristiques temporelles simples\n",
    "        temporal_stats = self._extract_temporal_stats(X)\n",
    "        \n",
    "        # 3. Ratios simples entre biomarqueurs\n",
    "        ratio_stats = self._extract_simple_ratios(X)\n",
    "        \n",
    "        # 4. Patterns de données manquantes basiques\n",
    "        missing_stats = self._extract_missing_stats(X)\n",
    "        \n",
    "        # Combiner\n",
    "        all_features = np.concatenate([basic_stats, temporal_stats, ratio_stats, missing_stats], axis=1)\n",
    "        \n",
    "        # Nettoyer\n",
    "        all_features = np.nan_to_num(all_features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        print(f\"✅ Caractéristiques créées : {all_features.shape[1]} (simple et efficace)\")\n",
    "        \n",
    "        return all_features\n",
    "    \n",
    "    def _extract_basic_stats(self, X):\n",
    "        \"\"\"Statistiques de base - les plus importantes\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        features = np.zeros((n_samples, n_features * 12))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_features):\n",
    "                # Extraire valeurs valides\n",
    "                values = []\n",
    "                for k in range(n_timesteps):\n",
    "                    val = X[i, k, j]\n",
    "                    if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                        try:\n",
    "                            values.append(float(val))\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if len(values) >= 3:\n",
    "                    values = np.array(values)\n",
    "                    \n",
    "                    # Statistiques importantes\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values)\n",
    "                    min_val = np.min(values)\n",
    "                    max_val = np.max(values)\n",
    "                    median_val = np.median(values)\n",
    "                    q25 = np.percentile(values, 25)\n",
    "                    q75 = np.percentile(values, 75)\n",
    "                    skewness = stats.skew(values)\n",
    "                    kurtosis = stats.kurtosis(values)\n",
    "                    range_val = max_val - min_val\n",
    "                    iqr = q75 - q25\n",
    "                    coeff_var = std_val / (abs(mean_val) + 1e-8)\n",
    "                    \n",
    "                elif len(values) > 0:\n",
    "                    values = np.array(values)\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values) if len(values) > 1 else 0\n",
    "                    min_val = np.min(values)\n",
    "                    max_val = np.max(values)\n",
    "                    median_val = mean_val\n",
    "                    q25 = q75 = mean_val\n",
    "                    skewness = kurtosis = 0\n",
    "                    range_val = iqr = coeff_var = 0\n",
    "                else:\n",
    "                    mean_val = std_val = min_val = max_val = 0\n",
    "                    median_val = q25 = q75 = skewness = kurtosis = 0\n",
    "                    range_val = iqr = coeff_var = 0\n",
    "                \n",
    "                start_idx = j * 12\n",
    "                features[i, start_idx:start_idx+12] = [\n",
    "                    mean_val, std_val, min_val, max_val, median_val, q25, q75,\n",
    "                    skewness, kurtosis, range_val, iqr, coeff_var\n",
    "                ]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_temporal_stats(self, X):\n",
    "        \"\"\"Caractéristiques temporelles importantes\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        features = np.zeros((n_samples, n_features * 8))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_features):\n",
    "                # Extraire valeurs valides\n",
    "                values = []\n",
    "                time_points = []\n",
    "                for k in range(n_timesteps):\n",
    "                    val = X[i, k, j]\n",
    "                    if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                        try:\n",
    "                            values.append(float(val))\n",
    "                            time_points.append(k)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if len(values) >= 3:\n",
    "                    values = np.array(values)\n",
    "                    time_points = np.array(time_points)\n",
    "                    \n",
    "                    # Tendance linéaire\n",
    "                    slope, intercept = np.polyfit(time_points, values, 1) if len(values) > 1 else (0, 0)\n",
    "                    \n",
    "                    # Accélération\n",
    "                    if len(values) >= 3:\n",
    "                        acceleration = np.polyfit(time_points, values, 2)[0]\n",
    "                    else:\n",
    "                        acceleration = 0\n",
    "                    \n",
    "                    # Volatilité\n",
    "                    volatility = np.std(values)\n",
    "                    \n",
    "                    # Première et dernière valeur\n",
    "                    first_val = values[0]\n",
    "                    last_val = values[-1]\n",
    "                    total_change = last_val - first_val\n",
    "                    \n",
    "                    # Changements de direction\n",
    "                    direction_changes = 0\n",
    "                    if len(values) >= 3:\n",
    "                        for k in range(1, len(values)-1):\n",
    "                            if (values[k] > values[k-1] and values[k+1] < values[k]) or \\\n",
    "                               (values[k] < values[k-1] and values[k+1] > values[k]):\n",
    "                                direction_changes += 1\n",
    "                    \n",
    "                    # R-squared de la tendance\n",
    "                    if len(values) > 2:\n",
    "                        predicted = slope * time_points + intercept\n",
    "                        r_squared = 1 - np.sum((values - predicted) ** 2) / np.sum((values - np.mean(values)) ** 2)\n",
    "                    else:\n",
    "                        r_squared = 0\n",
    "                        \n",
    "                else:\n",
    "                    slope = acceleration = volatility = 0\n",
    "                    first_val = last_val = total_change = 0\n",
    "                    direction_changes = r_squared = 0\n",
    "                \n",
    "                start_idx = j * 8\n",
    "                features[i, start_idx:start_idx+8] = [\n",
    "                    slope, acceleration, volatility, first_val, last_val,\n",
    "                    total_change, direction_changes, r_squared\n",
    "                ]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_simple_ratios(self, X):\n",
    "        \"\"\"Ratios simples entre biomarqueurs\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        \n",
    "        # Calculer moyennes des biomarqueurs\n",
    "        means = np.zeros((n_samples, n_features))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_features):\n",
    "                values = []\n",
    "                for k in range(n_timesteps):\n",
    "                    val = X[i, k, j]\n",
    "                    if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                        try:\n",
    "                            values.append(float(val))\n",
    "                        except:\n",
    "                            continue\n",
    "                means[i, j] = np.mean(values) if len(values) > 0 else 0\n",
    "        \n",
    "        # Groupes de biomarqueurs (comme dans le modèle original)\n",
    "        group_size = max(1, n_features // 4)\n",
    "        \n",
    "        glucose_group = np.mean(means[:, :group_size], axis=1) if group_size > 0 else np.zeros(n_samples)\n",
    "        lipid_group = np.mean(means[:, group_size:2*group_size], axis=1) if 2*group_size <= n_features else np.zeros(n_samples)\n",
    "        liver_group = np.mean(means[:, 2*group_size:3*group_size], axis=1) if 3*group_size <= n_features else np.zeros(n_samples)\n",
    "        kidney_group = np.mean(means[:, 3*group_size:], axis=1) if 3*group_size < n_features else np.zeros(n_samples)\n",
    "        \n",
    "        # Scores composites\n",
    "        metabolic_score = (glucose_group + lipid_group) / 2\n",
    "        organ_score = (liver_group + kidney_group) / 2\n",
    "        \n",
    "        # Ratios sécurisés\n",
    "        glucose_lipid_ratio = glucose_group / (np.abs(lipid_group) + 1e-8)\n",
    "        liver_kidney_ratio = liver_group / (np.abs(kidney_group) + 1e-8)\n",
    "        \n",
    "        ratios = np.column_stack([\n",
    "            glucose_group, lipid_group, liver_group, kidney_group,\n",
    "            metabolic_score, organ_score, glucose_lipid_ratio, liver_kidney_ratio\n",
    "        ])\n",
    "        \n",
    "        return ratios\n",
    "    \n",
    "    def _extract_missing_stats(self, X):\n",
    "        \"\"\"Statistiques de données manquantes\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        features = np.zeros((n_samples, 8))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            total_missing = 0\n",
    "            missing_per_timepoint = np.zeros(n_timesteps)\n",
    "            missing_per_feature = np.zeros(n_features)\n",
    "            \n",
    "            for j in range(n_timesteps):\n",
    "                for k in range(n_features):\n",
    "                    val = X[i, j, k]\n",
    "                    is_missing = val is None or (isinstance(val, float) and np.isnan(val))\n",
    "                    if is_missing:\n",
    "                        total_missing += 1\n",
    "                        missing_per_timepoint[j] += 1\n",
    "                        missing_per_feature[k] += 1\n",
    "            \n",
    "            missing_rate = total_missing / (n_timesteps * n_features)\n",
    "            max_missing_timepoint = np.max(missing_per_timepoint) / n_features\n",
    "            max_missing_feature = np.max(missing_per_feature) / n_timesteps\n",
    "            missing_timepoint_std = np.std(missing_per_timepoint)\n",
    "            missing_feature_std = np.std(missing_per_feature)\n",
    "            \n",
    "            early_missing = np.sum(missing_per_timepoint[:3]) / (3 * n_features) if n_timesteps >= 3 else 0\n",
    "            late_missing = np.sum(missing_per_timepoint[-3:]) / (3 * n_features) if n_timesteps >= 3 else 0\n",
    "            missing_concentration = np.var(missing_per_timepoint) + np.var(missing_per_feature)\n",
    "            \n",
    "            features[i] = [\n",
    "                missing_rate, max_missing_timepoint, max_missing_feature,\n",
    "                missing_timepoint_std, missing_feature_std, early_missing,\n",
    "                late_missing, missing_concentration\n",
    "            ]\n",
    "        \n",
    "        return features\n",
    "\n",
    "class SimpleBestPredictor:\n",
    "    \"\"\"\n",
    "    🚀 Prédicteur Simple utilisant les Meilleures Techniques\n",
    "    \n",
    "    Utilise exactement les mêmes paramètres qui ont donné F1=0.9442\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_engineer = SimpleAdvancedFeatureEngineer()\n",
    "        self.scaler = RobustScaler()\n",
    "        self.feature_selector = SelectKBest(f_classif, k=500)\n",
    "        self.model = None\n",
    "        self.best_threshold = 0.5\n",
    "        \n",
    "    def prepare_model(self):\n",
    "        \"\"\"Préparer le modèle avec les MÊMES paramètres qui ont marché\"\"\"\n",
    "        \n",
    "        # EXACTEMENT les mêmes paramètres qui ont donné F1=0.9442\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=150,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Modèle GradientBoosting avec paramètres optimaux préparé\")\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Entraînement avec la méthode qui a marché\"\"\"\n",
    "        \n",
    "        print(\"🎯 Entraînement avec les meilleures techniques...\")\n",
    "        \n",
    "        # Feature Engineering\n",
    "        X_engineered = self.feature_engineer.fit_transform(X)\n",
    "        \n",
    "        # Scaling (comme dans l'original)\n",
    "        X_scaled = self.scaler.fit_transform(X_engineered)\n",
    "        \n",
    "        # Feature Selection (comme dans l'original)\n",
    "        X_selected = self.feature_selector.fit_transform(X_scaled, y)\n",
    "        \n",
    "        print(f\"📊 Caractéristiques finales : {X_selected.shape[1]}\")\n",
    "        \n",
    "        # SMOTE (comme dans l'original)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_selected, y)\n",
    "        \n",
    "        print(f\"🔄 SMOTE : {len(y)} → {len(y_resampled)} échantillons\")\n",
    "        \n",
    "        # Entraîner\n",
    "        self.model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Validation\n",
    "        cv_scores = cross_val_score(\n",
    "            self.model, X_selected, y, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='f1'\n",
    "        )\n",
    "        \n",
    "        print(f\"🏆 CV F1 : {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "        \n",
    "        return X_selected\n",
    "    \n",
    "    def optimize_threshold(self, X_val, y_val):\n",
    "        \"\"\"Optimisation du seuil\"\"\"\n",
    "        \n",
    "        print(\"🎯 Optimisation du seuil...\")\n",
    "        \n",
    "        X_val_processed = self._transform_data(X_val)\n",
    "        y_proba = self.model.predict_proba(X_val_processed)[:, 1]\n",
    "        \n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_proba >= threshold).astype(int)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        self.best_threshold = best_threshold\n",
    "        print(f\"✅ Meilleur seuil : {best_threshold:.3f} (F1: {best_f1:.4f})\")\n",
    "        \n",
    "        return best_threshold, best_f1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Prédiction\"\"\"\n",
    "        X_processed = self._transform_data(X)\n",
    "        y_proba = self.model.predict_proba(X_processed)[:, 1]\n",
    "        y_pred = (y_proba >= self.best_threshold).astype(int)\n",
    "        return y_pred, y_proba\n",
    "    \n",
    "    def _transform_data(self, X):\n",
    "        \"\"\"Transformation\"\"\"\n",
    "        X_engineered = self.feature_engineer.transform(X)\n",
    "        X_scaled = self.scaler.transform(X_engineered)\n",
    "        X_selected = self.feature_selector.transform(X_scaled)\n",
    "        return X_selected\n",
    "\n",
    "def main():\n",
    "    \"\"\"Exécution avec les meilleures techniques\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"🚀 PRÉDICTEUR SIMPLE AVEC MEILLEURES TECHNIQUES\")\n",
    "    print(\"   Utilise exactement ce qui a donné F1=0.9442\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Charger données\n",
    "    print(\"📊 Chargement des données...\")\n",
    "    with np.load(\"../data/training_data.npz\", allow_pickle=True) as f:\n",
    "        X_train = f[\"data\"]\n",
    "        feature_names = f[\"feature_labels\"]\n",
    "    \n",
    "    y_train = pd.read_csv(\"../data/training_labels.csv\")[\"Label\"].values\n",
    "    \n",
    "    print(f\"✅ Données : {X_train.shape}\")\n",
    "    print(f\"✅ Étiquettes : {len(y_train)} (positives : {np.sum(y_train)} - {np.mean(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    # Division\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Initialisation\n",
    "    predictor = SimpleBestPredictor()\n",
    "    predictor.prepare_model()\n",
    "    \n",
    "    # Entraînement\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    predictor.train(X_train_split, y_train_split)\n",
    "    \n",
    "    # Optimisation seuil\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    best_threshold, best_f1 = predictor.optimize_threshold(X_val_split, y_val_split)\n",
    "    \n",
    "    # Évaluation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🎯 Évaluation finale :\")\n",
    "    \n",
    "    y_pred, y_proba = predictor.predict(X_val_split)\n",
    "    \n",
    "    f1 = f1_score(y_val_split, y_pred)\n",
    "    precision = precision_score(y_val_split, y_pred)\n",
    "    recall = recall_score(y_val_split, y_pred)\n",
    "    auc = roc_auc_score(y_val_split, y_proba)\n",
    "    \n",
    "    print(f\"🏆 Performance :\")\n",
    "    print(f\"   Score F1 : {f1:.4f}\")\n",
    "    print(f\"   Précision : {precision:.4f}\")\n",
    "    print(f\"   Rappel : {recall:.4f}\")\n",
    "    print(f\"   AUC-ROC : {auc:.4f}\")\n",
    "    \n",
    "    # Prédictions finales\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📤 Prédictions finales...\")\n",
    "    \n",
    "    with np.load(\"../data/evaluation_data.npz\", allow_pickle=True) as f:\n",
    "        X_test = f[\"data\"]\n",
    "    \n",
    "    y_test_pred, y_test_proba = predictor.predict(X_test)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'Id': range(len(y_test_pred)),\n",
    "        'Label': y_test_pred\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('submission_simple_best.csv', index=False)\n",
    "    \n",
    "    print(f\"✅ Fichier submission_simple_best.csv créé\")\n",
    "    print(f\"📊 Prédictions positives : {np.sum(y_test_pred)} ({np.mean(y_test_pred)*100:.1f}%)\")\n",
    "    print(f\"🏆 Score F1 : {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎉 Terminé ! Utilise les techniques qui ont donné F1=0.9442\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🚀 PRÉDICTEUR SIMPLE AVEC MEILLEURES TECHNIQUES\n",
      "   Utilise exactement ce qui a donné F1=0.9442\n",
      "======================================================================\n",
      "📊 Chargement des données...\n",
      "✅ Données : (53652, 12, 77)\n",
      "✅ Étiquettes : 53652 (positives : 3393 - 6.3%)\n",
      "✅ Modèle GradientBoosting avec paramètres optimaux préparé\n",
      "\n",
      "==================================================\n",
      "🎯 Entraînement avec les meilleures techniques...\n",
      "🔧 Feature Engineering Simple et Efficace...\n",
      "✅ Caractéristiques créées : 1556 (simple et efficace)\n",
      "📊 Caractéristiques finales : 500\n",
      "🔄 SMOTE : 42921 → 80414 échantillons\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 437\u001B[0m\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m70\u001B[39m)\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 437\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \n",
      "Cell \u001B[0;32mIn[4], line 389\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;66;03m# Entraînement\u001B[39;00m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m--> 389\u001B[0m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_split\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_split\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;66;03m# Optimisation seuil\u001B[39;00m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m50\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 310\u001B[0m, in \u001B[0;36mSimpleBestPredictor.train\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfit(X_resampled, y_resampled)\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m cv_scores \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_selected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStratifiedKFold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_splits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mf1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m    314\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m🏆 CV F1 : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(cv_scores)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ± \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mstd(cv_scores)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_selected\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    214\u001B[0m         )\n\u001B[1;32m    215\u001B[0m     ):\n\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    226\u001B[0m     )\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:684\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[1;32m    682\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[0;32m--> 684\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    214\u001B[0m         )\n\u001B[1;32m    215\u001B[0m     ):\n\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    226\u001B[0m     )\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:411\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[1;32m    410\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[0;32m--> 411\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    420\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    424\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    425\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[1;32m    433\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/joblib/parallel.py:1986\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1984\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1985\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1986\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1988\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1989\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1990\u001B[0m \u001B[38;5;66;03m# reused, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1991\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1992\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1993\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/joblib/parallel.py:1914\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1912\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1913\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1914\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1915\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1916\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:866\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    864\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 866\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:787\u001B[0m, in \u001B[0;36mBaseGradientBoosting.fit\u001B[0;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resize_state()\n\u001B[1;32m    786\u001B[0m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[0;32m--> 787\u001B[0m n_stages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_stages \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:883\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stages\u001B[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[0m\n\u001B[1;32m    876\u001B[0m         initial_loss \u001B[38;5;241m=\u001B[39m factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss(\n\u001B[1;32m    877\u001B[0m             y_true\u001B[38;5;241m=\u001B[39my_oob_masked,\n\u001B[1;32m    878\u001B[0m             raw_prediction\u001B[38;5;241m=\u001B[39mraw_predictions[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[1;32m    879\u001B[0m             sample_weight\u001B[38;5;241m=\u001B[39msample_weight_oob_masked,\n\u001B[1;32m    880\u001B[0m         )\n\u001B[1;32m    882\u001B[0m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[0;32m--> 883\u001B[0m raw_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[38;5;66;03m# track loss\u001B[39;00m\n\u001B[1;32m    896\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:489\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stage\u001B[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[0m\n\u001B[1;32m    486\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;241m*\u001B[39m sample_mask\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[1;32m    488\u001B[0m X \u001B[38;5;241m=\u001B[39m X_csc \u001B[38;5;28;01mif\u001B[39;00m X_csc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[0;32m--> 489\u001B[0m \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_g_view\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m    491\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[1;32m    494\u001B[0m X_for_tree_update \u001B[38;5;241m=\u001B[39m X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/tree/_classes.py:1404\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[0;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1376\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[1;32m   1377\u001B[0m \n\u001B[1;32m   1378\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1401\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[1;32m   1402\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1404\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1406\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1407\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1409\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    462\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[1;32m    463\u001B[0m         splitter,\n\u001B[1;32m    464\u001B[0m         min_samples_split,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[1;32m    470\u001B[0m     )\n\u001B[0;32m--> 472\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
