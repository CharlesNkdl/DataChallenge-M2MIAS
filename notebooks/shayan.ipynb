{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-02T13:49:24.360195Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SimpleAdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    🎯 Ingénieur de Caractéristiques Simple mais Efficace\n",
    "    \n",
    "    Utilise exactement les mêmes techniques qui ont donné F1=0.9442\n",
    "    mais de manière optimisée\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transformation simple mais efficace\"\"\"\n",
    "        print(\"🔧 Feature Engineering Simple et Efficace...\")\n",
    "        \n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        \n",
    "        # 1. Statistiques de base importantes (comme dans le modèle original)\n",
    "        basic_stats = self._extract_basic_stats(X)\n",
    "        \n",
    "        # 2. Caractéristiques temporelles simples\n",
    "        temporal_stats = self._extract_temporal_stats(X)\n",
    "        \n",
    "        # 3. Ratios simples entre biomarqueurs\n",
    "        ratio_stats = self._extract_simple_ratios(X)\n",
    "        \n",
    "        # 4. Patterns de données manquantes basiques\n",
    "        missing_stats = self._extract_missing_stats(X)\n",
    "        \n",
    "        # Combiner\n",
    "        all_features = np.concatenate([basic_stats, temporal_stats, ratio_stats, missing_stats], axis=1)\n",
    "        \n",
    "        # Nettoyer\n",
    "        all_features = np.nan_to_num(all_features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        print(f\"✅ Caractéristiques créées : {all_features.shape[1]} (simple et efficace)\")\n",
    "        \n",
    "        return all_features\n",
    "    \n",
    "    def _extract_basic_stats(self, X):\n",
    "        \"\"\"Statistiques de base - les plus importantes\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        features = np.zeros((n_samples, n_features * 12))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_features):\n",
    "                # Extraire valeurs valides\n",
    "                values = []\n",
    "                for k in range(n_timesteps):\n",
    "                    val = X[i, k, j]\n",
    "                    if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                        try:\n",
    "                            values.append(float(val))\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if len(values) >= 3:\n",
    "                    values = np.array(values)\n",
    "                    \n",
    "                    # Statistiques importantes\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values)\n",
    "                    min_val = np.min(values)\n",
    "                    max_val = np.max(values)\n",
    "                    median_val = np.median(values)\n",
    "                    q25 = np.percentile(values, 25)\n",
    "                    q75 = np.percentile(values, 75)\n",
    "                    skewness = stats.skew(values)\n",
    "                    kurtosis = stats.kurtosis(values)\n",
    "                    range_val = max_val - min_val\n",
    "                    iqr = q75 - q25\n",
    "                    coeff_var = std_val / (abs(mean_val) + 1e-8)\n",
    "                    \n",
    "                elif len(values) > 0:\n",
    "                    values = np.array(values)\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values) if len(values) > 1 else 0\n",
    "                    min_val = np.min(values)\n",
    "                    max_val = np.max(values)\n",
    "                    median_val = mean_val\n",
    "                    q25 = q75 = mean_val\n",
    "                    skewness = kurtosis = 0\n",
    "                    range_val = iqr = coeff_var = 0\n",
    "                else:\n",
    "                    mean_val = std_val = min_val = max_val = 0\n",
    "                    median_val = q25 = q75 = skewness = kurtosis = 0\n",
    "                    range_val = iqr = coeff_var = 0\n",
    "                \n",
    "                start_idx = j * 12\n",
    "                features[i, start_idx:start_idx+12] = [\n",
    "                    mean_val, std_val, min_val, max_val, median_val, q25, q75,\n",
    "                    skewness, kurtosis, range_val, iqr, coeff_var\n",
    "                ]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_temporal_stats(self, X):\n",
    "        \"\"\"Caractéristiques temporelles importantes\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        features = np.zeros((n_samples, n_features * 8))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_features):\n",
    "                # Extraire valeurs valides\n",
    "                values = []\n",
    "                time_points = []\n",
    "                for k in range(n_timesteps):\n",
    "                    val = X[i, k, j]\n",
    "                    if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                        try:\n",
    "                            values.append(float(val))\n",
    "                            time_points.append(k)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if len(values) >= 3:\n",
    "                    values = np.array(values)\n",
    "                    time_points = np.array(time_points)\n",
    "                    \n",
    "                    # Tendance linéaire\n",
    "                    slope, intercept = np.polyfit(time_points, values, 1) if len(values) > 1 else (0, 0)\n",
    "                    \n",
    "                    # Accélération\n",
    "                    if len(values) >= 3:\n",
    "                        acceleration = np.polyfit(time_points, values, 2)[0]\n",
    "                    else:\n",
    "                        acceleration = 0\n",
    "                    \n",
    "                    # Volatilité\n",
    "                    volatility = np.std(values)\n",
    "                    \n",
    "                    # Première et dernière valeur\n",
    "                    first_val = values[0]\n",
    "                    last_val = values[-1]\n",
    "                    total_change = last_val - first_val\n",
    "                    \n",
    "                    # Changements de direction\n",
    "                    direction_changes = 0\n",
    "                    if len(values) >= 3:\n",
    "                        for k in range(1, len(values)-1):\n",
    "                            if (values[k] > values[k-1] and values[k+1] < values[k]) or \\\n",
    "                               (values[k] < values[k-1] and values[k+1] > values[k]):\n",
    "                                direction_changes += 1\n",
    "                    \n",
    "                    # R-squared de la tendance\n",
    "                    if len(values) > 2:\n",
    "                        predicted = slope * time_points + intercept\n",
    "                        r_squared = 1 - np.sum((values - predicted) ** 2) / np.sum((values - np.mean(values)) ** 2)\n",
    "                    else:\n",
    "                        r_squared = 0\n",
    "                        \n",
    "                else:\n",
    "                    slope = acceleration = volatility = 0\n",
    "                    first_val = last_val = total_change = 0\n",
    "                    direction_changes = r_squared = 0\n",
    "                \n",
    "                start_idx = j * 8\n",
    "                features[i, start_idx:start_idx+8] = [\n",
    "                    slope, acceleration, volatility, first_val, last_val,\n",
    "                    total_change, direction_changes, r_squared\n",
    "                ]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_simple_ratios(self, X):\n",
    "        \"\"\"Ratios simples entre biomarqueurs\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        \n",
    "        # Calculer moyennes des biomarqueurs\n",
    "        means = np.zeros((n_samples, n_features))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_features):\n",
    "                values = []\n",
    "                for k in range(n_timesteps):\n",
    "                    val = X[i, k, j]\n",
    "                    if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                        try:\n",
    "                            values.append(float(val))\n",
    "                        except:\n",
    "                            continue\n",
    "                means[i, j] = np.mean(values) if len(values) > 0 else 0\n",
    "        \n",
    "        # Groupes de biomarqueurs (comme dans le modèle original)\n",
    "        group_size = max(1, n_features // 4)\n",
    "        \n",
    "        glucose_group = np.mean(means[:, :group_size], axis=1) if group_size > 0 else np.zeros(n_samples)\n",
    "        lipid_group = np.mean(means[:, group_size:2*group_size], axis=1) if 2*group_size <= n_features else np.zeros(n_samples)\n",
    "        liver_group = np.mean(means[:, 2*group_size:3*group_size], axis=1) if 3*group_size <= n_features else np.zeros(n_samples)\n",
    "        kidney_group = np.mean(means[:, 3*group_size:], axis=1) if 3*group_size < n_features else np.zeros(n_samples)\n",
    "        \n",
    "        # Scores composites\n",
    "        metabolic_score = (glucose_group + lipid_group) / 2\n",
    "        organ_score = (liver_group + kidney_group) / 2\n",
    "        \n",
    "        # Ratios sécurisés\n",
    "        glucose_lipid_ratio = glucose_group / (np.abs(lipid_group) + 1e-8)\n",
    "        liver_kidney_ratio = liver_group / (np.abs(kidney_group) + 1e-8)\n",
    "        \n",
    "        ratios = np.column_stack([\n",
    "            glucose_group, lipid_group, liver_group, kidney_group,\n",
    "            metabolic_score, organ_score, glucose_lipid_ratio, liver_kidney_ratio\n",
    "        ])\n",
    "        \n",
    "        return ratios\n",
    "    \n",
    "    def _extract_missing_stats(self, X):\n",
    "        \"\"\"Statistiques de données manquantes\"\"\"\n",
    "        n_samples, n_timesteps, n_features = X.shape\n",
    "        features = np.zeros((n_samples, 8))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            total_missing = 0\n",
    "            missing_per_timepoint = np.zeros(n_timesteps)\n",
    "            missing_per_feature = np.zeros(n_features)\n",
    "            \n",
    "            for j in range(n_timesteps):\n",
    "                for k in range(n_features):\n",
    "                    val = X[i, j, k]\n",
    "                    is_missing = val is None or (isinstance(val, float) and np.isnan(val))\n",
    "                    if is_missing:\n",
    "                        total_missing += 1\n",
    "                        missing_per_timepoint[j] += 1\n",
    "                        missing_per_feature[k] += 1\n",
    "            \n",
    "            missing_rate = total_missing / (n_timesteps * n_features)\n",
    "            max_missing_timepoint = np.max(missing_per_timepoint) / n_features\n",
    "            max_missing_feature = np.max(missing_per_feature) / n_timesteps\n",
    "            missing_timepoint_std = np.std(missing_per_timepoint)\n",
    "            missing_feature_std = np.std(missing_per_feature)\n",
    "            \n",
    "            early_missing = np.sum(missing_per_timepoint[:3]) / (3 * n_features) if n_timesteps >= 3 else 0\n",
    "            late_missing = np.sum(missing_per_timepoint[-3:]) / (3 * n_features) if n_timesteps >= 3 else 0\n",
    "            missing_concentration = np.var(missing_per_timepoint) + np.var(missing_per_feature)\n",
    "            \n",
    "            features[i] = [\n",
    "                missing_rate, max_missing_timepoint, max_missing_feature,\n",
    "                missing_timepoint_std, missing_feature_std, early_missing,\n",
    "                late_missing, missing_concentration\n",
    "            ]\n",
    "        \n",
    "        return features\n",
    "\n",
    "class SimpleBestPredictor:\n",
    "    \"\"\"\n",
    "    🚀 Prédicteur Simple utilisant les Meilleures Techniques\n",
    "    \n",
    "    Utilise exactement les mêmes paramètres qui ont donné F1=0.9442\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_engineer = SimpleAdvancedFeatureEngineer()\n",
    "        self.scaler = RobustScaler()\n",
    "        self.feature_selector = SelectKBest(f_classif, k=500)\n",
    "        self.model = None\n",
    "        self.best_threshold = 0.5\n",
    "        \n",
    "    def prepare_model(self):\n",
    "        \"\"\"Préparer le modèle avec les MÊMES paramètres qui ont marché\"\"\"\n",
    "        \n",
    "        # EXACTEMENT les mêmes paramètres qui ont donné F1=0.9442\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=150,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Modèle GradientBoosting avec paramètres optimaux préparé\")\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Entraînement avec la méthode qui a marché\"\"\"\n",
    "        \n",
    "        print(\"🎯 Entraînement avec les meilleures techniques...\")\n",
    "        \n",
    "        # Feature Engineering\n",
    "        X_engineered = self.feature_engineer.fit_transform(X)\n",
    "        \n",
    "        # Scaling (comme dans l'original)\n",
    "        X_scaled = self.scaler.fit_transform(X_engineered)\n",
    "        \n",
    "        # Feature Selection (comme dans l'original)\n",
    "        X_selected = self.feature_selector.fit_transform(X_scaled, y)\n",
    "        \n",
    "        print(f\"📊 Caractéristiques finales : {X_selected.shape[1]}\")\n",
    "        \n",
    "        # SMOTE (comme dans l'original)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_selected, y)\n",
    "        \n",
    "        print(f\"🔄 SMOTE : {len(y)} → {len(y_resampled)} échantillons\")\n",
    "        \n",
    "        # Entraîner\n",
    "        self.model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Validation\n",
    "        cv_scores = cross_val_score(\n",
    "            self.model, X_selected, y, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='f1'\n",
    "        )\n",
    "        \n",
    "        print(f\"🏆 CV F1 : {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "        \n",
    "        return X_selected\n",
    "    \n",
    "    def optimize_threshold(self, X_val, y_val):\n",
    "        \"\"\"Optimisation du seuil\"\"\"\n",
    "        \n",
    "        print(\"🎯 Optimisation du seuil...\")\n",
    "        \n",
    "        X_val_processed = self._transform_data(X_val)\n",
    "        y_proba = self.model.predict_proba(X_val_processed)[:, 1]\n",
    "        \n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_proba >= threshold).astype(int)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        self.best_threshold = best_threshold\n",
    "        print(f\"✅ Meilleur seuil : {best_threshold:.3f} (F1: {best_f1:.4f})\")\n",
    "        \n",
    "        return best_threshold, best_f1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Prédiction\"\"\"\n",
    "        X_processed = self._transform_data(X)\n",
    "        y_proba = self.model.predict_proba(X_processed)[:, 1]\n",
    "        y_pred = (y_proba >= self.best_threshold).astype(int)\n",
    "        return y_pred, y_proba\n",
    "    \n",
    "    def _transform_data(self, X):\n",
    "        \"\"\"Transformation\"\"\"\n",
    "        X_engineered = self.feature_engineer.transform(X)\n",
    "        X_scaled = self.scaler.transform(X_engineered)\n",
    "        X_selected = self.feature_selector.transform(X_scaled)\n",
    "        return X_selected\n",
    "\n",
    "def main():\n",
    "    \"\"\"Exécution avec les meilleures techniques\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"🚀 PRÉDICTEUR SIMPLE AVEC MEILLEURES TECHNIQUES\")\n",
    "    print(\"   Utilise exactement ce qui a donné F1=0.9442\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Charger données\n",
    "    print(\"📊 Chargement des données...\")\n",
    "    with np.load(\"../data/training_data.npz\", allow_pickle=True) as f:\n",
    "        X_train = f[\"data\"]\n",
    "        feature_names = f[\"feature_labels\"]\n",
    "    \n",
    "    y_train = pd.read_csv(\"../data/training_labels.csv\")[\"Label\"].values\n",
    "    \n",
    "    print(f\"✅ Données : {X_train.shape}\")\n",
    "    print(f\"✅ Étiquettes : {len(y_train)} (positives : {np.sum(y_train)} - {np.mean(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    # Division\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Initialisation\n",
    "    predictor = SimpleBestPredictor()\n",
    "    predictor.prepare_model()\n",
    "    \n",
    "    # Entraînement\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    predictor.train(X_train_split, y_train_split)\n",
    "    \n",
    "    # Optimisation seuil\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    best_threshold, best_f1 = predictor.optimize_threshold(X_val_split, y_val_split)\n",
    "    \n",
    "    # Évaluation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🎯 Évaluation finale :\")\n",
    "    \n",
    "    y_pred, y_proba = predictor.predict(X_val_split)\n",
    "    \n",
    "    f1 = f1_score(y_val_split, y_pred)\n",
    "    precision = precision_score(y_val_split, y_pred)\n",
    "    recall = recall_score(y_val_split, y_pred)\n",
    "    auc = roc_auc_score(y_val_split, y_proba)\n",
    "    \n",
    "    print(f\"🏆 Performance :\")\n",
    "    print(f\"   Score F1 : {f1:.4f}\")\n",
    "    print(f\"   Précision : {precision:.4f}\")\n",
    "    print(f\"   Rappel : {recall:.4f}\")\n",
    "    print(f\"   AUC-ROC : {auc:.4f}\")\n",
    "    \n",
    "    # Prédictions finales\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📤 Prédictions finales...\")\n",
    "    \n",
    "    with np.load(\"../data/evaluation_data.npz\", allow_pickle=True) as f:\n",
    "        X_test = f[\"data\"]\n",
    "    \n",
    "    y_test_pred, y_test_proba = predictor.predict(X_test)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'Id': range(len(y_test_pred)),\n",
    "        'Label': y_test_pred\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('submission_simple_best.csv', index=False)\n",
    "    \n",
    "    print(f\"✅ Fichier submission_simple_best.csv créé\")\n",
    "    print(f\"📊 Prédictions positives : {np.sum(y_test_pred)} ({np.mean(y_test_pred)*100:.1f}%)\")\n",
    "    print(f\"🏆 Score F1 : {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎉 Terminé ! Utilise les techniques qui ont donné F1=0.9442\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🚀 PRÉDICTEUR SIMPLE AVEC MEILLEURES TECHNIQUES\n",
      "   Utilise exactement ce qui a donné F1=0.9442\n",
      "======================================================================\n",
      "📊 Chargement des données...\n",
      "✅ Données : (53652, 12, 77)\n",
      "✅ Étiquettes : 53652 (positives : 3393 - 6.3%)\n",
      "✅ Modèle GradientBoosting avec paramètres optimaux préparé\n",
      "\n",
      "==================================================\n",
      "🎯 Entraînement avec les meilleures techniques...\n",
      "🔧 Feature Engineering Simple et Efficace...\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
