{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fd944",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Pipeline\n",
    "========\n",
    "1. Load data from NPZ / CSV.\n",
    "2. Feature‑wise standard scaling.\n",
    "3. 5‑fold stratified CV to create out‑of‑fold (OOF) predictions.\n",
    "4. Inside each fold:\n",
    "   • 1‑D CNN on the raw 12×77 tensor (GPU if available).\n",
    "   • LightGBM on handcrafted stats (mean/std/min/max/slope).\n",
    "   • CatBoost on the flattened raw sequence (12×77 = 924 features).\n",
    "5. Blend the three probability vectors (simple average).\n",
    "6. Find the global threshold on OOF to maximise F1.\n",
    "7. Apply the threshold to evaluation probabilities and write `submission.csv`.\n",
    "\n",
    "Run\n",
    "---\n",
    "```bash\n",
    "pip install numpy pandas scikit-learn torch lightgbm catboost\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "import os, random, time, math, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "SEED = 98\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "DATA_DIR      = Path('.')          # edit if your files are elsewhere\n",
    "TRAIN_NPZ     = DATA_DIR / '/kaggle/input/mias-m2-t2d-data-challenge-2025/training_data.npz' #à modifier\n",
    "TRAIN_LABEL_CSV = DATA_DIR / '/kaggle/input/mias-m2-t2d-data-challenge-2025/training_labels.csv' #à modifier\n",
    "EVAL_NPZ      = DATA_DIR / '/kaggle/input/mias-m2-t2d-data-challenge-2025/evaluation_data.npz' #à modifier\n",
    "SUBMISSION_CSV = DATA_DIR / 'submissionV2.csv'\n",
    "N_SPLITS      = 5\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray | None = None):\n",
    "        self.x = torch.from_numpy(x)\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx]) if self.y is not None else self.x[idx]\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_feat: int = 77, n_filt: int = 96):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_feat, n_filt, 3, padding=1),\n",
    "            nn.BatchNorm1d(n_filt), nn.ReLU(),\n",
    "            nn.Conv1d(n_filt, n_filt, 3, padding=1),\n",
    "            nn.BatchNorm1d(n_filt), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(n_filt, 1)\n",
    "    def forward(self, x):              # x: (B, T, F)\n",
    "        x = x.permute(0, 2, 1)         # -> (B, F, T)\n",
    "        x = self.conv(x).squeeze(-1)   # -> (B, n_filt)\n",
    "        return self.fc(x).squeeze(-1)  # logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def summary_stats(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return mean, std, min, max, and slope along time axis (axis=1).\"\"\"\n",
    "    mean  = x.mean(1)\n",
    "    std   = x.std(1)\n",
    "    mn    = x.min(1)\n",
    "    mx    = x.max(1)\n",
    "    slope = x[:, -1] - x[:, 0]\n",
    "    return np.hstack([mean, std, mn, mx, slope])\n",
    "\n",
    "\n",
    "def best_threshold(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"Grid‑search threshold ∈ (0.05,0.95) for max F1.\"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 37)\n",
    "    f1_scores = [f1_score(y_true, y_prob >= t) for t in thresholds]\n",
    "    best_idx = int(np.argmax(f1_scores))\n",
    "    return float(thresholds[best_idx]), float(f1_scores[best_idx])\n",
    "\n",
    "\n",
    "def train_cnn_fold(x_train: np.ndarray, y_train: np.ndarray,\n",
    "                   x_val: np.ndarray,   y_val: np.ndarray,\n",
    "                   epochs: int = 25, batch: int = 256) -> Tuple[nn.Module, np.ndarray, np.ndarray]:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model  = CNN1D().to(device)\n",
    "    pos_weight = torch.tensor([(len(y_train) - y_train.sum()) / y_train.sum()], device=device)\n",
    "    criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer  = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    tr_dl = DataLoader(TensorDataset(x_train, y_train), batch_size=batch, shuffle=True)\n",
    "    va_dl = DataLoader(TensorDataset(x_val,   y_val),   batch_size=batch)\n",
    "\n",
    "    best_auc = 0.\n",
    "    best_weights = model.state_dict()  # ensure dict‑like even if no epoch improves\n",
    "    patience = 5; bad_epochs = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in tr_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb.to(device))\n",
    "            loss = criterion(logits, yb.to(device))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "            optimizer.step()\n",
    "\n",
    "        # ----- validation -----\n",
    "        model.eval(); y_prob = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in va_dl:\n",
    "                p = torch.sigmoid(model(xb.to(device))).cpu()\n",
    "                y_prob.append(p)\n",
    "        y_prob = torch.cat(y_prob).numpy()\n",
    "        auc = roc_auc_score(y_val, y_prob)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc; bad_epochs = 0\n",
    "            best_weights = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            break\n",
    "    # load best weights\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # final predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = torch.sigmoid(model(torch.from_numpy(x_val).to(device))).cpu().numpy()\n",
    "        train_pred = torch.sigmoid(model(torch.from_numpy(x_train).to(device))).cpu().numpy()\n",
    "    return model, val_pred, train_pred\n",
    "\n",
    "\n",
    "def train_lightgbm(x_tr: np.ndarray, y_tr: np.ndarray) -> lgb.Booster:\n",
    "    pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum()\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.05,\n",
    "        'num_leaves': 64, 'min_data_in_leaf': 50,\n",
    "        'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 1, 'seed': SEED,\n",
    "        'scale_pos_weight': pos_weight,\n",
    "    }\n",
    "    dtrain = lgb.Dataset(x_tr, y_tr)\n",
    "    gbm = lgb.train(params, dtrain, num_boost_round=800)\n",
    "    return gbm\n",
    "\n",
    "\n",
    "def train_catboost(x_tr: np.ndarray, y_tr: np.ndarray) -> CatBoostClassifier:\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function='Logloss', eval_metric='AUC', learning_rate=0.05,\n",
    "        depth=6, l2_leaf_reg=3.0, random_seed=SEED, verbose=False,\n",
    "        iterations=600, class_weights=[1.0, (len(y_tr)/y_tr.sum())])\n",
    "    model.fit(x_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # ----- load data -----\n",
    "    with np.load(TRAIN_NPZ, allow_pickle=True) as f:\n",
    "        X = f['data'].astype(np.float32)\n",
    "        feat_labels = f['feature_labels']\n",
    "    y = pd.read_csv(TRAIN_LABEL_CSV)['Label'].values\n",
    "\n",
    "    with np.load(EVAL_NPZ, allow_pickle=True) as f:\n",
    "        X_eval = f['data'].astype(np.float32)\n",
    "\n",
    "    print(f\"Loaded train shape {X.shape}  | positives {y.sum()/len(y):.2%}\")\n",
    "\n",
    "    # ----- imputation + scaling -----\n",
    "    # 1. median‑impute each biomarker (handles missing lab values)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler  = StandardScaler()\n",
    "\n",
    "    X_2d        = X.reshape(-1, X.shape[-1])   # (N*T, 77)\n",
    "    X_eval_2d   = X_eval.reshape(-1, X_eval.shape[-1])\n",
    "\n",
    "    X_2d        = imputer.fit_transform(X_2d)\n",
    "    X_eval_2d   = imputer.transform(X_eval_2d)\n",
    "\n",
    "    X_2d        = scaler.fit_transform(X_2d)\n",
    "    X_eval_2d   = scaler.transform(X_eval_2d)\n",
    "\n",
    "    # Replace any NaN / ±Inf produced by constant biomarkers (std=0)\n",
    "    X_2d      = np.nan_to_num(X_2d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_eval_2d = np.nan_to_num(X_eval_2d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    X       = X_2d.reshape(X.shape)\n",
    "    X_eval  = X_eval_2d.reshape(X_eval.shape)\n",
    "\n",
    "    # ----- CV setup -----\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_prob = np.zeros(len(y), dtype=float)\n",
    "    eval_prob = np.zeros(len(X_eval), dtype=float)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\nFold {fold}/{N_SPLITS}  (train {len(train_idx)}, val {len(val_idx)})\")\n",
    "\n",
    "        # split\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # ---------------- CNN ----------------\n",
    "        cnn_model, val_cnn, _ = train_cnn_fold(X_tr, y_tr, X_val, y_val)\n",
    "        with torch.no_grad():\n",
    "            device = next(cnn_model.parameters()).device\n",
    "            eval_cnn = []\n",
    "            for i in range(0, len(X_eval), 512):\n",
    "                xb = torch.from_numpy(X_eval[i:i+512]).to(device)\n",
    "                eval_cnn.append(torch.sigmoid(cnn_model(xb)).cpu())\n",
    "            eval_cnn = torch.cat(eval_cnn).numpy()\n",
    "\n",
    "        # ---------------- LightGBM -----------\n",
    "        X_tr_stat, X_val_stat, X_eval_stat = summary_stats(X_tr), summary_stats(X_val), summary_stats(X_eval)\n",
    "        gbm = train_lightgbm(X_tr_stat, y_tr)\n",
    "        val_gbm = gbm.predict(X_val_stat, num_iteration=gbm.best_iteration)\n",
    "        eval_gbm = gbm.predict(X_eval_stat, num_iteration=gbm.best_iteration)\n",
    "\n",
    "        # ---------------- CatBoost -----------\n",
    "        X_tr_flat = X_tr.reshape(len(X_tr), -1); X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "        X_eval_flat = X_eval.reshape(len(X_eval), -1)\n",
    "        cb = train_catboost(X_tr_flat, y_tr)\n",
    "        val_cb = cb.predict_proba(X_val_flat)[:, 1]\n",
    "        eval_cb = cb.predict_proba(X_eval_flat)[:, 1]\n",
    "\n",
    "        # ---------------- Blend --------------\n",
    "        val_blend = (val_cnn + val_gbm + val_cb) / 3\n",
    "        eval_blend_fold = (eval_cnn + eval_gbm + eval_cb) / 3\n",
    "\n",
    "        oof_prob[val_idx] = val_blend\n",
    "        eval_prob += eval_blend_fold / N_SPLITS\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val, val_blend)\n",
    "        fold_thr, fold_f1 = best_threshold(y_val, val_blend)\n",
    "        print(f\"Fold AUC={fold_auc:.4f} | best F1={fold_f1:.4f} @thr={fold_thr:.3f}\")\n",
    "\n",
    "    # ----- global threshold & metrics -----\n",
    "    thr_star, f1_star = best_threshold(y, oof_prob)\n",
    "    auc_star = roc_auc_score(y, oof_prob)\n",
    "    print(f\"\\nOOF AUC={auc_star:.4f} | OOF F1={f1_star:.4f} @thr={thr_star:.3f}\")\n",
    "\n",
    "    # ----- submission -----\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': np.arange(len(eval_prob)),\n",
    "        'Label': (eval_prob >= thr_star).astype(int)   # binary as per competition spec\n",
    "        # For probabilistic submission replace by: eval_prob\n",
    "    })\n",
    "    submission.to_csv(SUBMISSION_CSV, index=False)\n",
    "    print(f\"Wrote {SUBMISSION_CSV}  (rows {len(submission)})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
