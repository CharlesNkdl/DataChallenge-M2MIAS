{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Pipeline\n",
    "========\n",
    "1. Load data from NPZ / CSV.\n",
    "2. Feature‑wise standard scaling.\n",
    "3. 5‑fold stratified CV to create out‑of‑fold (OOF) predictions.\n",
    "4. Inside each fold:\n",
    "   • 1‑D CNN on the raw 12×77 tensor (GPU if available).\n",
    "   • LightGBM on handcrafted stats (mean/std/min/max/slope).\n",
    "   • CatBoost on the flattened raw sequence (12×77 = 924 features).\n",
    "5. Blend the three probability vectors (simple average).\n",
    "6. Find the global threshold on OOF to maximise F1.\n",
    "7. Apply the threshold to evaluation probabilities and write `submission.csv`.\n",
    "\n",
    "Run\n",
    "---\n",
    "```bash\n",
    "pip install numpy pandas scikit-learn torch lightgbm catboost\n",
    "```"
   ],
   "id": "1a5362756a5b05b4"
  },
  {
   "cell_type": "code",
   "id": "de3fd944",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2025-06-04T14:41:21.459889Z",
     "start_time": "2025-06-04T14:41:04.511495Z"
    }
   },
   "source": [
    "import os, random, time, math, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "SEED = 19980311\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "DATA_DIR      = Path('../data/')          # edit if your files are elsewhere\n",
    "TRAIN_NPZ     = DATA_DIR / 'training_data.npz' #à modifier\n",
    "TRAIN_LABEL_CSV = DATA_DIR / 'training_labels.csv' #à modifier\n",
    "EVAL_NPZ      = DATA_DIR / 'evaluation_data.npz' #à modifier\n",
    "SUBMISSION_CSV = DATA_DIR / 'submissionV2.csv'\n",
    "N_SPLITS      = 3\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray | None = None):\n",
    "        self.x = torch.from_numpy(x)\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx]) if self.y is not None else self.x[idx]\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_feat: int = 77, n_filt: int = 96):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_feat, n_filt, 3, padding=1),\n",
    "            nn.BatchNorm1d(n_filt), nn.ReLU(),\n",
    "            nn.Conv1d(n_filt, n_filt, 3, padding=1),\n",
    "            nn.BatchNorm1d(n_filt), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(n_filt, 1)\n",
    "    def forward(self, x):              # x: (B, T, F)\n",
    "        x = x.permute(0, 2, 1)         # -> (B, F, T)\n",
    "        x = self.conv(x).squeeze(-1)   # -> (B, n_filt)\n",
    "        return self.fc(x).squeeze(-1)  # logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def summary_stats(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return mean, std, min, max, and slope along time axis (axis=1).\"\"\"\n",
    "    mean  = x.mean(1)\n",
    "    std   = x.std(1)\n",
    "    mn    = x.min(1)\n",
    "    mx    = x.max(1)\n",
    "    slope = x[:, -1] - x[:, 0]\n",
    "    return np.hstack([mean, std, mn, mx, slope])\n",
    "\n",
    "\n",
    "def best_threshold(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"Grid‑search threshold ∈ (0.05,0.95) for max F1.\"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 37)\n",
    "    f1_scores = [f1_score(y_true, y_prob >= t) for t in thresholds]\n",
    "    best_idx = int(np.argmax(f1_scores))\n",
    "    return float(thresholds[best_idx]), float(f1_scores[best_idx])\n",
    "\n",
    "\n",
    "def train_cnn_fold(x_train: np.ndarray, y_train: np.ndarray,\n",
    "                   x_val: np.ndarray,   y_val: np.ndarray,\n",
    "                   epochs: int = 25, batch: int = 64) -> Tuple[nn.Module, np.ndarray, np.ndarray]:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model  = CNN1D().to(device)\n",
    "    pos_weight = torch.tensor([(len(y_train) - y_train.sum()) / y_train.sum()], device=device)\n",
    "    criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer  = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    tr_dl = DataLoader(TensorDataset(x_train, y_train), batch_size=batch, shuffle=True)\n",
    "    va_dl = DataLoader(TensorDataset(x_val,   y_val),   batch_size=batch)\n",
    "\n",
    "    best_auc = 0.\n",
    "    best_weights = model.state_dict()  # ensure dict‑like even if no epoch improves\n",
    "    patience = 5; bad_epochs = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in tr_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb.to(device))\n",
    "            loss = criterion(logits, yb.to(device))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "            optimizer.step()\n",
    "\n",
    "        # ----- validation -----\n",
    "        model.eval(); y_prob = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in va_dl:\n",
    "                p = torch.sigmoid(model(xb.to(device))).cpu()\n",
    "                y_prob.append(p)\n",
    "        y_prob = torch.cat(y_prob).numpy()\n",
    "        auc = roc_auc_score(y_val, y_prob)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc; bad_epochs = 0\n",
    "            best_weights = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            break\n",
    "    # load best weights\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # final predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = torch.sigmoid(model(torch.from_numpy(x_val).to(device))).cpu().numpy()\n",
    "        train_pred = torch.sigmoid(model(torch.from_numpy(x_train).to(device))).cpu().numpy()\n",
    "    return model, val_pred, train_pred\n",
    "\n",
    "\n",
    "def train_lightgbm(x_tr: np.ndarray, y_tr: np.ndarray) -> lgb.Booster:\n",
    "    pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum()\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.05,\n",
    "        'num_leaves': 32, 'min_data_in_leaf': 50,\n",
    "        'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 1, 'seed': SEED,\n",
    "        'scale_pos_weight': pos_weight,\n",
    "    }\n",
    "    dtrain = lgb.Dataset(x_tr, y_tr)\n",
    "    gbm = lgb.train(params, dtrain, num_boost_round=800)\n",
    "    return gbm\n",
    "\n",
    "\n",
    "def train_catboost(x_tr: np.ndarray, y_tr: np.ndarray) -> CatBoostClassifier:\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function='Logloss', eval_metric='AUC', learning_rate=0.05,\n",
    "        depth=6, l2_leaf_reg=3.0, random_seed=SEED, verbose=False,\n",
    "        iterations=600, class_weights=[1.0, (len(y_tr)/y_tr.sum())])\n",
    "    model.fit(x_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # ----- load data -----\n",
    "    with np.load(TRAIN_NPZ, allow_pickle=True) as f:\n",
    "        X = f['data'].astype(np.float32)\n",
    "        feat_labels = f['feature_labels']\n",
    "    y = pd.read_csv(TRAIN_LABEL_CSV)['Label'].values\n",
    "\n",
    "    with np.load(EVAL_NPZ, allow_pickle=True) as f:\n",
    "        X_eval = f['data'].astype(np.float32)\n",
    "\n",
    "    print(f\"Loaded train shape {X.shape}  | positives {y.sum()/len(y):.2%}\")\n",
    "\n",
    "    # ----- imputation + scaling -----\n",
    "    # 1. median‑impute each biomarker (handles missing lab values)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler  = StandardScaler()\n",
    "\n",
    "    X_2d        = X.reshape(-1, X.shape[-1])   # (N*T, 77)\n",
    "    X_eval_2d   = X_eval.reshape(-1, X_eval.shape[-1])\n",
    "\n",
    "    X_2d        = imputer.fit_transform(X_2d)\n",
    "    X_eval_2d   = imputer.transform(X_eval_2d)\n",
    "\n",
    "    X_2d        = scaler.fit_transform(X_2d)\n",
    "    X_eval_2d   = scaler.transform(X_eval_2d)\n",
    "\n",
    "    # Replace any NaN / ±Inf produced by constant biomarkers (std=0)\n",
    "    X_2d      = np.nan_to_num(X_2d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_eval_2d = np.nan_to_num(X_eval_2d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    X       = X_2d.reshape(X.shape)\n",
    "    X_eval  = X_eval_2d.reshape(X_eval.shape)\n",
    "\n",
    "    # ----- CV setup -----\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_prob = np.zeros(len(y), dtype=float)\n",
    "    eval_prob = np.zeros(len(X_eval), dtype=float)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\nFold {fold}/{N_SPLITS}  (train {len(train_idx)}, val {len(val_idx)})\")\n",
    "\n",
    "        # split\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # ---------------- CNN ----------------\n",
    "        cnn_model, val_cnn, _ = train_cnn_fold(X_tr, y_tr, X_val, y_val)\n",
    "        with torch.no_grad():\n",
    "            device = next(cnn_model.parameters()).device\n",
    "            eval_cnn = []\n",
    "            for i in range(0, len(X_eval), 512):\n",
    "                xb = torch.from_numpy(X_eval[i:i+512]).to(device)\n",
    "                eval_cnn.append(torch.sigmoid(cnn_model(xb)).cpu())\n",
    "            eval_cnn = torch.cat(eval_cnn).numpy()\n",
    "\n",
    "        # ---------------- LightGBM -----------\n",
    "        X_tr_stat, X_val_stat, X_eval_stat = summary_stats(X_tr), summary_stats(X_val), summary_stats(X_eval)\n",
    "        gbm = train_lightgbm(X_tr_stat, y_tr)\n",
    "        val_gbm = gbm.predict(X_val_stat, num_iteration=gbm.best_iteration)\n",
    "        eval_gbm = gbm.predict(X_eval_stat, num_iteration=gbm.best_iteration)\n",
    "\n",
    "        # ---------------- CatBoost -----------\n",
    "        X_tr_flat = X_tr.reshape(len(X_tr), -1); X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "        X_eval_flat = X_eval.reshape(len(X_eval), -1)\n",
    "        cb = train_catboost(X_tr_flat, y_tr)\n",
    "        val_cb = cb.predict_proba(X_val_flat)[:, 1]\n",
    "        eval_cb = cb.predict_proba(X_eval_flat)[:, 1]\n",
    "\n",
    "        # ---------------- Blend --------------\n",
    "        val_blend = (val_cnn + val_gbm + val_cb) / 3\n",
    "        eval_blend_fold = (eval_cnn + eval_gbm + eval_cb) / 3\n",
    "\n",
    "        oof_prob[val_idx] = val_blend\n",
    "        eval_prob += eval_blend_fold / N_SPLITS\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val, val_blend)\n",
    "        fold_thr, fold_f1 = best_threshold(y_val, val_blend)\n",
    "        print(f\"Fold AUC={fold_auc:.4f} | best F1={fold_f1:.4f} @thr={fold_thr:.3f}\")\n",
    "\n",
    "    # ----- global threshold & metrics -----\n",
    "    thr_star, f1_star = best_threshold(y, oof_prob)\n",
    "    auc_star = roc_auc_score(y, oof_prob)\n",
    "    print(f\"\\nOOF AUC={auc_star:.4f} | OOF F1={f1_star:.4f} @thr={thr_star:.3f}\")\n",
    "\n",
    "    # ----- submission -----\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': np.arange(len(eval_prob)),\n",
    "        'Label': (eval_prob >= thr_star).astype(int)   \n",
    "    })\n",
    "    submission.to_csv(SUBMISSION_CSV, index=False)\n",
    "    print(f\"Wrote {SUBMISSION_CSV}  (rows {len(submission)})\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train shape (53652, 12, 77)  | positives 6.32%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 180\u001B[0m\n\u001B[1;32m    177\u001B[0m X_2d        \u001B[38;5;241m=\u001B[39m imputer\u001B[38;5;241m.\u001B[39mfit_transform(X_2d)\n\u001B[1;32m    178\u001B[0m X_eval_2d   \u001B[38;5;241m=\u001B[39m imputer\u001B[38;5;241m.\u001B[39mtransform(X_eval_2d)\n\u001B[0;32m--> 180\u001B[0m X_2d        \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_2d\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m X_eval_2d   \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mtransform(X_eval_2d)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;66;03m# Replace any NaN / ±Inf produced by constant biomarkers (std=0)\u001B[39;00m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 319\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    322\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    323\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    324\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    325\u001B[0m         )\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/base.py:918\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    903\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    904\u001B[0m             (\n\u001B[1;32m    905\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) has a `transform`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    913\u001B[0m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[1;32m    914\u001B[0m         )\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    917\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    920\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:894\u001B[0m, in \u001B[0;36mStandardScaler.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    892\u001B[0m \u001B[38;5;66;03m# Reset internal state before fitting\u001B[39;00m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 894\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1016\u001B[0m, in \u001B[0;36mStandardScaler.partial_fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1013\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_seen_ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39misnan(X)\u001B[38;5;241m.\u001B[39msum(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   1015\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1016\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvar_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_seen_ \u001B[38;5;241m=\u001B[39m \u001B[43m_incremental_mean_and_var\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m            \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvar_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_samples_seen_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m            \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001B[39;00m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001B[39;00m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;66;03m# missing values)\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mptp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_seen_) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/extmath.py:1121\u001B[0m, in \u001B[0;36m_incremental_mean_and_var\u001B[0;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001B[0m\n\u001B[1;32m   1119\u001B[0m     correction \u001B[38;5;241m=\u001B[39m _safe_accumulator_op(sum_op, temp, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   1120\u001B[0m     temp \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m-> 1121\u001B[0m     new_unnormalized_variance \u001B[38;5;241m=\u001B[39m \u001B[43m_safe_accumulator_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43msum_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;66;03m# correction term of the corrected 2 pass algorithm.\u001B[39;00m\n\u001B[1;32m   1124\u001B[0m \u001B[38;5;66;03m# See \"Algorithms for computing the sample variance: analysis\u001B[39;00m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;66;03m# and recommendations\", by Chan, Golub, and LeVeque.\u001B[39;00m\n\u001B[1;32m   1126\u001B[0m new_unnormalized_variance \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m correction\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m new_sample_count\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/sklearn/utils/extmath.py:1017\u001B[0m, in \u001B[0;36m_safe_accumulator_op\u001B[0;34m(op, x, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1015\u001B[0m     result \u001B[38;5;241m=\u001B[39m op(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1017\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:2466\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2463\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2464\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2466\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2467\u001B[0m \u001B[43m    \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\n\u001B[1;32m   2469\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
