{
 "cells": [
  {
   "cell_type": "code",
   "id": "b1421a28aa39dec3",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T13:03:06.459357Z",
     "start_time": "2025-06-02T13:02:55.129800Z"
    }
   },
   "source": [
    "# # Analyse et Prédiction du Diabète Type 2\n",
    "# Objectif: Prédire l'apparition du T2D dans l'année suivante\n",
    "\n",
    "# ## 1. Import des librairies et chargement des données\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8') # or 'seaborn-v0_8-whitegrid' if you prefer\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Librairies importées avec succès\")\n",
    "\n",
    "# ## 2. Chargement et exploration des données\n",
    "\n",
    "# --- MODIFICATION HERE ---\n",
    "print(\"Chargement des données...\")\n",
    "\n",
    "# Define paths (replace with actual paths if different)\n",
    "TRAINING_DATA_PATH = '../data/training_data.npz'\n",
    "TRAINING_LABELS_PATH = '../data/training_labels.csv'\n",
    "EVALUATION_DATA_PATH = '../data/evaluation_data.npz'\n",
    "SAMPLE_SUBMISSION_PATH = '../data/sample_submission.csv'\n",
    "\n",
    "# Load training data and feature names\n",
    "with np.load(TRAINING_DATA_PATH, allow_pickle=True) as f:\n",
    "    X_train = f[\"data\"]\n",
    "    feature_names_original = f[\"feature_labels\"]\n",
    "\n",
    "# Load training labels\n",
    "training_labels_df = pd.read_csv(TRAINING_LABELS_PATH)\n",
    "y_train = training_labels_df['Label'].values\n",
    "\n",
    "# Load evaluation data\n",
    "with np.load(EVALUATION_DATA_PATH, allow_pickle=True) as f:\n",
    "    X_eval = f[\"data\"]\n",
    "    # Assuming evaluation data has the same feature structure,\n",
    "    # feature_names_original from training can be used if needed for eval feature engineering.\n",
    "\n",
    "# Load sample submission\n",
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "# --- END OF MODIFICATION ---"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librairies importées avec succès\n",
      "Chargement des données...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:06:15.905887Z",
     "start_time": "2025-06-02T13:06:15.897093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(f\"Noms des features originales (exemple): {feature_names_original[:5]}\") # Print a few original feature names\n",
    "print(f\"Nombre de features originales: {len(feature_names_original)}\")\n",
    "print(f\"Données d'entraînement: {X_train.shape}\")\n",
    "print(f\"Labels d'entraînement: {y_train.shape}\")\n",
    "print(f\"Données d'évaluation: {X_eval.shape}\")\n",
    "print(f\"Distribution des classes: {np.bincount(y_train)}\")\n",
    "print(f\"Pourcentage de cas positifs: {np.mean(y_train)*100:.2f}%\")\n"
   ],
   "id": "379e0f4ea57b8ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noms des features originales (exemple): ['creatinine' 'urea' 'c_reactive_protein' 'prothrombin_time'\n",
      " 'total_proteins']\n",
      "Nombre de features originales: 77\n",
      "Données d'entraînement: (53652, 12, 77)\n",
      "Labels d'entraînement: (53652,)\n",
      "Données d'évaluation: (26826, 12, 77)\n",
      "Distribution des classes: [50259  3393]\n",
      "Pourcentage de cas positifs: 6.32%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:09:45.701260Z",
     "start_time": "2025-06-02T13:09:37.860796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Supposons que X_train, feature_names_original, X_eval, y_train sont déjà chargés\n",
    "# comme dans le script complet que vous avez fourni précédemment.\n",
    "\n",
    "# Idéalement, la conversion de dtype suivante devrait être faite\n",
    "# immédiatement après le chargement de X_train et X_eval.\n",
    "# On le place ici pour le contexte de ce snippet.\n",
    "\n",
    "if 'X_train' in locals() and X_train.dtype == object:\n",
    "    print(f\"Type de données initial de X_train: {X_train.dtype}\")\n",
    "    print(\"X_train est de type 'object'. Tentative de conversion en np.float64...\")\n",
    "    try:\n",
    "        X_train = X_train.astype(np.float64)\n",
    "        print(f\"Conversion de X_train en np.float64 réussie. Nouveau dtype: {X_train.dtype}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERREUR: Échec de la conversion de X_train en np.float64: {e}\")\n",
    "        print(\"Cela peut se produire si l'array contient des chaînes de caractères ou d'autres objets non convertibles en float.\")\n",
    "        print(\"L'analyse risque d'échouer. Vérifiez le contenu de votre fichier .npz.\")\n",
    "        # Ici, une logique de conversion plus robuste pourrait être nécessaire si l'erreur persiste\n",
    "        # par exemple, remplacer manuellement des chaînes comme 'NA', 'missing' par np.nan avant astype.\n",
    "\n",
    "# Assurez-vous que X_eval est aussi converti si vous l'utilisez plus tard avec des opérations numériques.\n",
    "# if 'X_eval' in locals() and X_eval.dtype == object:\n",
    "# print(f\"Type de données initial de X_eval: {X_eval.dtype}\")\n",
    "# print(\"X_eval est de type 'object'. Tentative de conversion en np.float64...\")\n",
    "# try:\n",
    "# X_eval = X_eval.astype(np.float64)\n",
    "# print(f\"Conversion de X_eval en np.float64 réussie. Nouveau dtype: {X_eval.dtype}\")\n",
    "# except ValueError as e:\n",
    "# print(f\"ERREUR: Échec de la conversion de X_eval en np.float64: {e}\")\n",
    "\n",
    "\n",
    "# Vérification des valeurs manquantes et outliers\n",
    "print(\"\\n=== ANALYSE DE LA QUALITÉ DES DONNÉES ===\")\n",
    "# Ces opérations devraient maintenant fonctionner si X_train est de type numérique\n",
    "print(f\"Valeurs NaN dans X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Valeurs infinies dans X_train: {np.isinf(X_train).sum()}\")\n",
    "# Les valeurs négatives peuvent être valides pour certaines features (ex: changement de valeur)\n",
    "# mais il est bon de vérifier si c'est inattendu pour des résultats de laboratoire typiquement positifs.\n",
    "# Cette opération nécessite aussi un type numérique.\n",
    "print(f\"Valeurs négatives (potentiellement problématiques): {(X_train < 0).sum() if X_train.dtype != object else 'Non calculé (X_train est object)'}\")\n",
    "\n",
    "# Statistiques par feature\n",
    "feature_stats = []\n",
    "# S'assurer que feature_names_original est défini et a la bonne longueur\n",
    "if 'feature_names_original' not in locals() or len(feature_names_original) != X_train.shape[2]:\n",
    "    print(\"Attention: 'feature_names_original' n'est pas défini ou ne correspond pas au nombre de features. Utilisation de noms génériques.\")\n",
    "    feature_names_original = [f\"feature_{i}\" for i in range(X_train.shape[2])]\n",
    "\n",
    "for feat_idx in range(X_train.shape[2]):\n",
    "    # .flatten() crée une copie, donc pas de souci de modification de X_train ici.\n",
    "    feat_data_all_timesteps = X_train[:, :, feat_idx].flatten()\n",
    "\n",
    "    # Exclure les NaN pour les calculs de mean, std, min, max\n",
    "    feat_data_no_nan = feat_data_all_timesteps[~np.isnan(feat_data_all_timesteps)]\n",
    "\n",
    "    if len(feat_data_no_nan) > 0:\n",
    "        mean_val = np.mean(feat_data_no_nan)\n",
    "        std_val = np.std(feat_data_no_nan)\n",
    "        min_val = np.min(feat_data_no_nan)\n",
    "        max_val = np.max(feat_data_no_nan)\n",
    "    else: # Toutes les valeurs pour cette feature sont NaN\n",
    "        mean_val, std_val, min_val, max_val = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    stats_dict = {\n",
    "        'feature_idx': feat_idx,\n",
    "        'feature_name': feature_names_original[feat_idx],\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'nan_rate': np.isnan(feat_data_all_timesteps).mean(), # Taux de NaN sur toutes les valeurs de la feature\n",
    "        # Pour zero_rate, np.nan == 0 est False, donc les NaNs ne sont pas comptés comme zéros.\n",
    "        'zero_rate': (feat_data_all_timesteps == 0).mean()\n",
    "    }\n",
    "    feature_stats.append(stats_dict)\n",
    "\n",
    "feature_stats_df = pd.DataFrame(feature_stats)\n",
    "print(\"\\nStatistiques descriptives par feature (calculées sur les valeurs non-NaN pour mean/std/min/max):\")\n",
    "print(feature_stats_df.head().to_string()) # .to_string() pour un meilleur affichage si les noms sont longs\n",
    "print(f\"\\nTaux moyen de valeurs manquantes par feature: {feature_stats_df['nan_rate'].mean():.4f}\")\n",
    "print(f\"Taux moyen de zéros par feature: {feature_stats_df['zero_rate'].mean():.4f}\")\n",
    "\n",
    "# Identifier les features avec un taux élevé de NaN\n",
    "high_nan_threshold = 0.8\n",
    "high_nan_features = feature_stats_df[feature_stats_df['nan_rate'] > high_nan_threshold]\n",
    "print(f\"\\nFeatures avec plus de {high_nan_threshold*100:.0f}% de NaN ({len(high_nan_features)}):\")\n",
    "if not high_nan_features.empty:\n",
    "    print(high_nan_features[['feature_name', 'nan_rate']].to_string(index=False))\n",
    "else:\n",
    "    print(f\"Aucune feature n'a plus de {high_nan_threshold*100:.0f}% de NaN.\")"
   ],
   "id": "a15c22a898546934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type de données initial de X_train: object\n",
      "X_train est de type 'object'. Tentative de conversion en np.float64...\n",
      "Conversion de X_train en np.float64 réussie. Nouveau dtype: float64\n",
      "\n",
      "=== ANALYSE DE LA QUALITÉ DES DONNÉES ===\n",
      "Valeurs NaN dans X_train: 48050049\n",
      "Valeurs infinies dans X_train: 0\n",
      "Valeurs négatives (potentiellement problématiques): 0\n",
      "\n",
      "Statistiques descriptives par feature (calculées sur les valeurs non-NaN pour mean/std/min/max):\n",
      "   feature_idx        feature_name         mean          std        min           max  nan_rate  zero_rate\n",
      "0            0          creatinine    10.293623    61.165592   0.000000   9532.000000  0.943660   0.000002\n",
      "1            1                urea     0.661896    25.556834   0.000000   2869.086667  0.980429   0.000002\n",
      "2            2  c_reactive_protein  1722.509578  3639.069854   0.000000   9532.000000  0.962918   0.000002\n",
      "3            3    prothrombin_time    91.968974   248.683332  13.000000  21642.000000  0.988286   0.000000\n",
      "4            4      total_proteins    70.552531    29.052826  34.800001   1764.940000  0.994458   0.000000\n",
      "\n",
      "Taux moyen de valeurs manquantes par feature: 0.9693\n",
      "Taux moyen de zéros par feature: 0.0009\n",
      "\n",
      "Features avec plus de 80% de NaN (77):\n",
      "                        feature_name  nan_rate\n",
      "                          creatinine  0.943660\n",
      "                                urea  0.980429\n",
      "                  c_reactive_protein  0.962918\n",
      "                    prothrombin_time  0.988286\n",
      "                      total_proteins  0.994458\n",
      "                                 ast  0.960222\n",
      "                                 alt  0.960222\n",
      "                alkaline_phosphatase  0.983126\n",
      "                             albumin  0.991027\n",
      "                     total_bilirubin  0.987975\n",
      "                                 ggt  0.969790\n",
      "              unconjugated_bilirubin  0.987989\n",
      "                conjugated_bilirubin  0.987989\n",
      "                                 ldh  0.997467\n",
      "                            ferritin  0.982602\n",
      "                          serum_iron  0.993897\n",
      "              transferrin_saturation  0.995483\n",
      "                         transferrin  0.995441\n",
      "                           uric_acid  0.986638\n",
      "                    prothrombin_time  0.999804\n",
      "                     creatine_kinase  0.997068\n",
      "                 parathyroid_hormone  0.999199\n",
      "                vitamin_d_25oh_total  0.994314\n",
      "                     fasting_glucose  0.999351\n",
      "                        serum_folate  0.996367\n",
      "                         vitamin_b12  0.996435\n",
      "                     fasting_glucose  0.959411\n",
      "                                 tsh  0.972432\n",
      "                            anti_hbs  0.997050\n",
      "                            anti_hcv  0.996101\n",
      "                               hba1c  0.984872\n",
      "                beta_2_microglobulin  0.999786\n",
      "                             glucose  0.999548\n",
      "         high_sensitivity_troponin_t  0.998869\n",
      "                           total_psa  0.992894\n",
      "                                 igg  0.999696\n",
      "                                 iga  0.999466\n",
      "                                 igm  0.999699\n",
      "                               hba1c  0.999727\n",
      "estimated_glomerular_filtration_rate  0.949047\n",
      "                   corrected_calcium  0.994536\n",
      "                                 inr  0.992270\n",
      "                               inr_a  0.999775\n",
      "                           potassium  0.949058\n",
      "                              sodium  0.950698\n",
      "                            chloride  0.953330\n",
      "                           total_co2  0.998088\n",
      "                             calcium  0.987933\n",
      "                          phosphorus  0.995076\n",
      "                           magnesium  0.998503\n",
      "                          hematocrit  0.933434\n",
      "                           platelets  0.930697\n",
      "                        plasma_cells  0.933516\n",
      "                                 rdw  0.933514\n",
      "                         neutrophils  0.933491\n",
      "                         lymphocytes  0.933491\n",
      "                                mchc  0.934504\n",
      "                           monocytes  0.933491\n",
      "                          myelocytes  0.933516\n",
      "                granular_lymphocytes  0.933516\n",
      "                       erythroblasts  0.933516\n",
      "                          hemoglobin  0.933490\n",
      "                   white_blood_cells  0.933490\n",
      "         hyperbasophilic_lymphocytes  0.933516\n",
      "                     red_blood_cells  0.933490\n",
      "                 villous_lymphocytes  0.933516\n",
      "                atypical_lymphocytes  0.933516\n",
      "                                 mch  0.934504\n",
      "                                 mcv  0.934171\n",
      "                       reticulocytes  0.998557\n",
      "                   total_cholesterol  0.966966\n",
      "                 non_hdl_cholesterol  0.968672\n",
      "                     hdl_cholesterol  0.968597\n",
      "                       triglycerides  0.966589\n",
      "            measured_ldl_cholesterol  0.968726\n",
      "                         patient_age  0.878885\n",
      "                                 sex  0.878885\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:21:02.009510Z",
     "start_time": "2025-06-02T13:15:48.792632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ## 3. Feature Engineering et Extraction de Features Temporelles\n",
    "\n",
    "# --- MODIFICATION HERE ---\n",
    "def extract_temporal_features(X, original_feature_names_list=None):\n",
    "    \"\"\"\n",
    "    Extrait des features temporelles à partir des séries de 12 mois\n",
    "    Utilise original_feature_names_list pour nommer les features dérivées.\n",
    "    \"\"\"\n",
    "    n_samples, n_timesteps, n_features = X.shape\n",
    "\n",
    "    # Initialisation des features extraites\n",
    "    features_list = []\n",
    "    engineered_feature_names_list = [] # Renamed for clarity\n",
    "\n",
    "    for feat_idx in range(n_features):\n",
    "        # Use original feature name if available, otherwise use index\n",
    "        base_feat_name = f\"origfeat_{feat_idx}\"\n",
    "        if original_feature_names_list is not None and feat_idx < len(original_feature_names_list):\n",
    "            base_feat_name = original_feature_names_list[feat_idx]\n",
    "        # Sanitize feature name for use in column names (e.g. remove spaces, special chars)\n",
    "        base_feat_name = base_feat_name.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "        # Données pour cette feature\n",
    "        feat_data = X[:, :, feat_idx]  # Shape: (n_samples, 12)\n",
    "\n",
    "        # 1. Statistiques de base\n",
    "        features_list.append(np.nanmean(feat_data, axis=1))  # Moyenne sur 12 mois, ignorant les NaN\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_mean\")\n",
    "\n",
    "        features_list.append(np.nanstd(feat_data, axis=1))   # Écart-type, ignorant les NaN\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_std\")\n",
    "\n",
    "        features_list.append(np.nanmin(feat_data, axis=1))   # Minimum, ignorant les NaN\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_min\")\n",
    "\n",
    "        features_list.append(np.nanmax(feat_data, axis=1))   # Maximum, ignorant les NaN\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_max\")\n",
    "\n",
    "        # 2. Tendance (pente de régression linéaire)\n",
    "        slopes = []\n",
    "        x_reg = np.arange(n_timesteps)\n",
    "        for i in range(n_samples):\n",
    "            y_reg = feat_data[i]\n",
    "            valid_indices = ~np.isnan(y_reg)\n",
    "            if np.sum(valid_indices) >= 2: # Need at least 2 points for linregress\n",
    "                try:\n",
    "                    # stats.linregress can't handle NaNs directly\n",
    "                    slope, _, _, _, _ = stats.linregress(x_reg[valid_indices], y_reg[valid_indices])\n",
    "                    slopes.append(slope)\n",
    "                except ValueError: # Catch potential errors if data is still problematic\n",
    "                    slopes.append(np.nan) # Use np.nan for failed calculations\n",
    "            else:\n",
    "                slopes.append(np.nan) # Use np.nan if not enough data points\n",
    "\n",
    "        features_list.append(np.array(slopes))\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_slope\")\n",
    "\n",
    "        # 3. Coefficient de variation (handle potential division by zero or nan mean)\n",
    "        mean_val = np.nanmean(feat_data, axis=1)\n",
    "        std_val = np.nanstd(feat_data, axis=1)\n",
    "        cv = np.divide(std_val, mean_val + 1e-8, out=np.full_like(mean_val, np.nan), where=mean_val!=0) # Add 1e-8 to mean for stability\n",
    "        features_list.append(cv)\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_cv\")\n",
    "\n",
    "        # 4. Différence entre derniers et premiers 3 mois\n",
    "        # Use nanmean to be robust to NaNs within the 3-month windows\n",
    "        first_3 = np.nanmean(feat_data[:, :3], axis=1)\n",
    "        last_3 = np.nanmean(feat_data[:, -3:], axis=1)\n",
    "        delta = last_3 - first_3\n",
    "        features_list.append(delta)\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_delta_3m\")\n",
    "\n",
    "        # 5. Moyenne mobile 3 mois (variance of MA)\n",
    "        ma3_std_values = []\n",
    "        for i in range(n_samples):\n",
    "            patient_ma3_series = []\n",
    "            # Calculate rolling mean if enough non-NaN points\n",
    "            for t in range(n_timesteps - 2): # iterate up to the point where a 3-month window can start\n",
    "                window = feat_data[i, t:t+3]\n",
    "                if np.sum(~np.isnan(window)) > 0: # if at least one non-NaN value in window\n",
    "                    patient_ma3_series.append(np.nanmean(window))\n",
    "                else:\n",
    "                    patient_ma3_series.append(np.nan)\n",
    "\n",
    "            if len(patient_ma3_series) > 1 and np.sum(~np.isnan(patient_ma3_series)) > 1:\n",
    "                 ma3_std_values.append(np.nanstd(patient_ma3_series))\n",
    "            else:\n",
    "                ma3_std_values.append(np.nan)\n",
    "\n",
    "        features_list.append(np.array(ma3_std_values))\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_ma3_std\")\n",
    "        \n",
    "        # 6. Last known value (often very predictive)\n",
    "        last_values = []\n",
    "        for i in range(n_samples):\n",
    "            patient_series = feat_data[i, :]\n",
    "            valid_lv = patient_series[~np.isnan(patient_series)]\n",
    "            if len(valid_lv) > 0:\n",
    "                last_values.append(valid_lv[-1])\n",
    "            else:\n",
    "                last_values.append(np.nan)\n",
    "        features_list.append(np.array(last_values))\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_last_value\")\n",
    "\n",
    "        # 7. Number of non-NaN measurements\n",
    "        features_list.append(np.sum(~np.isnan(feat_data), axis=1))\n",
    "        engineered_feature_names_list.append(f\"{base_feat_name}_num_measurements\")\n"
   ],
   "id": "ef8013703425c94f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    # Conversion en array\n",
    "    X_engineered = np.column_stack(features_list)\n",
    "\n",
    "    print(f\"Features originales: {n_features}\")\n",
    "    print(f\"Features extraites par feature originale: {X_engineered.shape[1] // n_features if n_features > 0 else 0}\")\n",
    "    print(f\"Total features extraites: {X_engineered.shape[1]}\")\n",
    "\n",
    "    return X_engineered, engineered_feature_names_list\n",
    "# --- END OF MODIFICATION ---\n"
   ],
   "id": "5fa7b24e5d24b91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE ENGINEERING ===\n",
      "Features originales: 77\n",
      "Features extraites par feature originale: 10\n",
      "Total features extraites: 770\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 126\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# Pass the original feature names to the function\u001B[39;00m\n\u001B[1;32m    125\u001B[0m X_train_engineered, engineered_feature_names \u001B[38;5;241m=\u001B[39m extract_temporal_features(X_train, feature_names_original)\n\u001B[0;32m--> 126\u001B[0m X_eval_engineered, _ \u001B[38;5;241m=\u001B[39m \u001B[43mextract_temporal_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_eval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names_original\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Use same names for consistency\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape après feature engineering (entraînement): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX_train_engineered\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape après feature engineering (évaluation): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX_eval_engineered\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[14], line 28\u001B[0m, in \u001B[0;36mextract_temporal_features\u001B[0;34m(X, original_feature_names_list)\u001B[0m\n\u001B[1;32m     25\u001B[0m feat_data \u001B[38;5;241m=\u001B[39m X[:, :, feat_idx]  \u001B[38;5;66;03m# Shape: (n_samples, 12)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# 1. Statistiques de base\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m features_list\u001B[38;5;241m.\u001B[39mappend(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnanmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeat_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# Moyenne sur 12 mois, ignorant les NaN\u001B[39;00m\n\u001B[1;32m     29\u001B[0m engineered_feature_names_list\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_feat_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     31\u001B[0m features_list\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mnanstd(feat_data, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))   \u001B[38;5;66;03m# Écart-type, ignorant les NaN\u001B[39;00m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1047\u001B[0m, in \u001B[0;36mnanmean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m   1043\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf a is inexact, then out must be inexact\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1045\u001B[0m cnt \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m~\u001B[39mmask, axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mintp, keepdims\u001B[38;5;241m=\u001B[39mkeepdims,\n\u001B[1;32m   1046\u001B[0m              where\u001B[38;5;241m=\u001B[39mwhere)\n\u001B[0;32m-> 1047\u001B[0m tot \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1048\u001B[0m \u001B[43m             \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1049\u001B[0m avg \u001B[38;5;241m=\u001B[39m _divide_by_count(tot, cnt, out\u001B[38;5;241m=\u001B[39mout)\n\u001B[1;32m   1051\u001B[0m isbad \u001B[38;5;241m=\u001B[39m (cnt \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:2466\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2463\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2464\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2466\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2467\u001B[0m \u001B[43m    \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\n\u001B[1;32m   2469\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/challenge/DataChall/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "\n",
    "# Extraction des features temporelles\n",
    "print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "# Pass the original feature names to the function\n",
    "X_train_engineered, engineered_feature_names = extract_temporal_features(X_train, feature_names_original)\n",
    "X_eval_engineered, _ = extract_temporal_features(X_eval, feature_names_original) # Use same names for consistency\n",
    "\n",
    "print(f\"Shape après feature engineering (entraînement): {X_train_engineered.shape}\")\n",
    "print(f\"Shape après feature engineering (évaluation): {X_eval_engineered.shape}\")\n",
    "print(f\"Quelques noms de features ingéniérées: {engineered_feature_names[:10]}\")\n",
    "\n",
    "\n",
    "# Gestion des valeurs manquantes et normalisation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"\\n=== PREPROCESSING ===\")\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "# Using median is generally robust. Consider different strategies or feature-specific imputation.\n",
    "imputer = SimpleImputer(strategy='median') # Using median for robustness to outliers\n",
    "X_train_imputed = imputer.fit_transform(X_train_engineered)\n",
    "X_eval_imputed = imputer.transform(X_eval_engineered)\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_eval_scaled = scaler.transform(X_eval_imputed)\n",
    "\n",
    "print(f\"Données après preprocessing (entraînement): {X_train_scaled.shape}\")\n",
    "print(f\"Valeurs manquantes après imputation (entraînement): {np.isnan(X_train_scaled).sum()}\")\n",
    "print(f\"Valeurs infinies après imputation (entraînement): {np.isinf(X_train_scaled).sum()}\")"
   ],
   "id": "88f9a3b35fed650"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# ## 4. Analyse de l'importance des features\n",
    "def analyze_feature_importance(X, y, current_feature_names, n_top=20):\n",
    "    \"\"\"\n",
    "    Analyse l'importance des features avec plusieurs méthodes\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ANALYSE D'IMPORTANCE DES FEATURES ===\")\n",
    "    # 1. Test univarié (f_classif)\n",
    "    print(\"--- Analyse Univariée (f_classif) ---\")\n",
    "    # Ensure no NaN/inf values are passed to SelectKBest\n",
    "    if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
    "        print(\"Warning: NaN or Inf values detected in X before SelectKBest. This can cause errors.\")\n",
    "        # A more robust solution would be to re-impute or handle them here.\n",
    "        # For now, we assume previous imputation handled this.\n",
    "        \n",
    "    selector_univariate = SelectKBest(f_classif, k='all')\n",
    "    selector_univariate.fit(X, y)\n",
    "    \n",
    "    univariate_scores = selector_univariate.scores_\n",
    "    univariate_pvalues = selector_univariate.pvalues_\n",
    "    \n",
    "    # Handle potential NaN scores/pvalues from f_classif (e.g., if a feature is constant)\n",
    "    univariate_scores = np.nan_to_num(univariate_scores, nan=0.0)\n",
    "    univariate_pvalues = np.nan_to_num(univariate_pvalues, nan=1.0) # Assign p-value of 1 to non-significant\n",
    "\n",
    "    # 2. Random Forest Feature Importance\n",
    "    print(\"--- Importance via Random Forest ---\")\n",
    "    rf_importance_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    rf_importance_model.fit(X, y)\n",
    "    rf_importance = rf_importance_model.feature_importances_\n",
    "    \n",
    "    # Création du DataFrame de résultats\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature_name': current_feature_names,\n",
    "        'univariate_score': univariate_scores,\n",
    "        'univariate_pvalue': univariate_pvalues,\n",
    "        'rf_importance': rf_importance\n",
    "    })\n",
    "    \n",
    "    # Normalisation des scores pour comparaison (MinMaxScaler like)\n",
    "    for col in ['univariate_score', 'rf_importance']:\n",
    "        min_val = importance_df[col].min()\n",
    "        max_val = importance_df[col].max()\n",
    "        if max_val == min_val: # Avoid division by zero if all values are the same\n",
    "            importance_df[f'{col}_norm'] = 0.0 if max_val == 0 else 1.0\n",
    "        else:\n",
    "            importance_df[f'{col}_norm'] = (importance_df[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Score composite (pondération ajustable)\n",
    "    # Giving more weight to RF importance as it considers interactions.\n",
    "    # Penalizing high p-values from univariate test.\n",
    "    importance_df['composite_score'] = (\n",
    "        importance_df['univariate_score_norm'] * 0.3 + # Weight for univariate F-score\n",
    "        importance_df['rf_importance_norm'] * 0.7    # Weight for RF importance\n",
    "        # (1 - importance_df['univariate_pvalue']) # Optional: factor in p-value directly\n",
    "    )\n",
    "    \n",
    "    # Tri par score composite\n",
    "    importance_df = importance_df.sort_values('composite_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyse de l'importance\n",
    "# Make sure engineered_feature_names has the correct length\n",
    "if X_train_scaled.shape[1] != len(engineered_feature_names):\n",
    "    print(f\"Warning: Mismatch in number of scaled features ({X_train_scaled.shape[1]}) and engineered feature names ({len(engineered_feature_names)}).\")\n",
    "    # Fallback if names are mismatched (should not happen if extract_temporal_features is correct)\n",
    "    engineered_feature_names_corrected = [f\"eng_feat_{i}\" for i in range(X_train_scaled.shape[1])]\n",
    "    importance_results = analyze_feature_importance(X_train_scaled, y_train, engineered_feature_names_corrected)\n",
    "else:\n",
    "    importance_results = analyze_feature_importance(X_train_scaled, y_train, engineered_feature_names)\n",
    "\n",
    "\n",
    "# Affichage des top features\n",
    "print(\"\\n=== TOP 20 FEATURES (basé sur score composite) ===\")\n",
    "top_features_display = importance_results.head(20)\n",
    "print(top_features_display[['feature_name', 'composite_score', 'univariate_score', 'rf_importance', 'univariate_pvalue']].to_string(index=False))\n",
    "\n",
    "# Visualisation de l'importance des features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 8)) # Changed to 1 row, 3 cols for better display\n",
    "plt.subplots_adjust(wspace=0.4) # Add more space between plots\n",
    "\n",
    "top_n_vis = 15 # Number of features to visualize\n",
    "\n",
    "# Univariate F-score\n",
    "top_univariate = importance_results.sort_values('univariate_score', ascending=False).head(top_n_vis)\n",
    "axes[0].barh(range(len(top_univariate)), top_univariate['univariate_score'], color=sns.color_palette(\"viridis\", top_n_vis))\n",
    "axes[0].set_yticks(range(len(top_univariate)))\n",
    "axes[0].set_yticklabels([name[:30] + '...' if len(name) > 30 else name for name in top_univariate['feature_name']], fontsize=8)\n",
    "axes[0].set_title(f'Top {top_n_vis} Features - Univarié (F-score)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Random Forest Importance\n",
    "top_rf = importance_results.sort_values('rf_importance', ascending=False).head(top_n_vis)\n",
    "axes[1].barh(range(len(top_rf)), top_rf['rf_importance'], color=sns.color_palette(\"magma\", top_n_vis))\n",
    "axes[1].set_yticks(range(len(top_rf)))\n",
    "axes[1].set_yticklabels([name[:30] + '...' if len(name) > 30 else name for name in top_rf['feature_name']], fontsize=8)\n",
    "axes[1].set_title(f'Top {top_n_vis} Features - Random Forest')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Composite Score\n",
    "top_composite_vis = importance_results.head(top_n_vis) # Already sorted by composite_score\n",
    "axes[2].barh(range(len(top_composite_vis)), top_composite_vis['composite_score'], color=sns.color_palette(\"cubehelix\", top_n_vis))\n",
    "axes[2].set_yticks(range(len(top_composite_vis)))\n",
    "axes[2].set_yticklabels([name[:30] + '...' if len(name) > 30 else name for name in top_composite_vis['feature_name']], fontsize=8)\n",
    "axes[2].set_title(f'Top {top_n_vis} Features - Score Composite')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "b8f50231a9272657"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ## 5. Sélection des meilleures features\n",
    "\n",
    "# Sélection des top features pour la modélisation\n",
    "# Can be based on a fixed number, a threshold on importance score, or p-value\n",
    "n_selected_features = min(100, X_train_scaled.shape[1]) # Ensure not to select more features than available\n",
    "\n",
    "# It's better to select features based on their original index in the scaled array,\n",
    "# which corresponds to their order in `importance_results` if it was not re-indexed improperly.\n",
    "# `importance_results` is already sorted by 'composite_score' descending.\n",
    "# So, the indices of the top N features are simply the first N indices of the *original* feature set\n",
    "# *if the importance_df was created with correct indices matching X_train_scaled columns*.\n",
    "\n",
    "# The `importance_df` contains feature names. We need to map these names back to indices of X_train_scaled.\n",
    "# `engineered_feature_names` holds the names in the order they appear as columns in `X_train_scaled`.\n",
    "selected_feature_names_ordered = importance_results.head(n_selected_features)['feature_name'].tolist()\n",
    "\n",
    "# Get the indices of these selected features from the original list of engineered feature names\n",
    "# This ensures we select the correct columns from X_train_scaled\n",
    "selected_features_indices = [engineered_feature_names.index(name) for name in selected_feature_names_ordered if name in engineered_feature_names]\n",
    "\n",
    "# Check if all selected names were found\n",
    "if len(selected_features_indices) != n_selected_features:\n",
    "    print(f\"Warning: Could only find {len(selected_features_indices)} out of {n_selected_features} selected feature names. This might indicate an issue.\")\n",
    "    # Fallback or error handling might be needed here. For now, proceed with found features.\n",
    "\n",
    "X_train_selected = X_train_scaled[:, selected_features_indices]\n",
    "X_eval_selected = X_eval_scaled[:, selected_features_indices]\n",
    "\n",
    "print(f\"\\n=== SÉLECTION DE FEATURES ===\")\n",
    "print(f\"Nombre de features sélectionnées: {X_train_selected.shape[1]}\")\n",
    "print(f\"Shape finale pour entraînement: {X_train_selected.shape}\")\n",
    "print(\"Top 10 des features sélectionnées:\")\n",
    "for i, name in enumerate(selected_feature_names_ordered[:10]):\n",
    "    print(f\"  {i+1}. {name} (Composite Score: {importance_results.iloc[i]['composite_score']:.4f})\")\n",
    "\n",
    "\n",
    "# Analyse de corrélation entre top features (e.g., top 20 selected)\n",
    "n_corr_features = min(20, X_train_selected.shape[1])\n",
    "if n_corr_features > 1: # Need at least 2 features for correlation\n",
    "    correlation_matrix = np.corrcoef(X_train_selected[:, :n_corr_features].T)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', annot_kws={\"size\": 8}, cbar_kws={\"shrink\": .8})\n",
    "    plt.title(f'Matrice de Corrélation - Top {n_corr_features} Features Sélectionnées')\n",
    "    \n",
    "    tick_labels = [name[:20] + '...' if len(name) > 20 else name for name in selected_feature_names_ordered[:n_corr_features]]\n",
    "    plt.xticks(np.arange(n_corr_features) + 0.5, tick_labels, rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(np.arange(n_corr_features) + 0.5, tick_labels, rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Moins de 2 features sélectionnées, impossible d'afficher la matrice de corrélation.\")\n"
   ],
   "id": "26f9d5afcdc52aa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ## 6. Modélisation et évaluation\n",
    "\n",
    "# Division train/validation avec stratification\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_selected, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train # Important for imbalanced datasets\n",
    ")\n",
    "\n",
    "print(f\"\\n=== DIVISION DES DONNÉES POUR MODÉLISATION ===\")\n",
    "print(f\"Train final: {X_train_final.shape}, Positifs: {np.mean(y_train_final)*100:.2f}%\")\n",
    "print(f\"Validation: {X_val.shape}, Positifs: {np.mean(y_val)*100:.2f}%\")\n",
    "\n",
    "# --- MODEL TRAINING ---\n",
    "models_dict = {} # To store trained models and their results\n",
    "\n",
    "# 1. Random Forest\n",
    "print(\"\\n=== ENTRAÎNEMENT RANDOM FOREST ===\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,      # Number of trees\n",
    "    max_depth=10,          # Max depth of trees\n",
    "    min_samples_split=10,  # Min samples to split a node\n",
    "    min_samples_leaf=5,    # Min samples in a leaf node\n",
    "    class_weight='balanced', # Adjusts weights inversely proportional to class frequencies\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # Use all available cores\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "rf_pred_proba_val = rf_model.predict_proba(X_val)[:, 1] # Probabilities for the positive class\n",
    "rf_pred_val = (rf_pred_proba_val > 0.5).astype(int) # Predictions based on 0.5 threshold\n",
    "\n",
    "rf_auc_val = roc_auc_score(y_val, rf_pred_proba_val)\n",
    "print(f\"Random Forest AUC (Validation): {rf_auc_val:.4f}\")\n",
    "print(\"\\nRandom Forest Classification Report (Validation):\")\n",
    "print(classification_report(y_val, rf_pred_val, target_names=['Non-Diabétique', 'Diabétique']))\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, rf_pred_val))\n",
    "\n",
    "models_dict['Random Forest'] = {'model': rf_model, 'proba_val': rf_pred_proba_val, 'auc_val': rf_auc_val}\n",
    "\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print(\"\\n=== ENTRAÎNEMENT LOGISTIC REGRESSION ===\")\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced', # Handles class imbalance\n",
    "    random_state=42,\n",
    "    solver='liblinear',      # Good for smaller datasets, handles L1/L2\n",
    "    C=1.0,                   # Inverse of regularization strength\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_final, y_train_final)\n",
    "lr_pred_proba_val = lr_model.predict_proba(X_val)[:, 1]\n",
    "lr_pred_val = (lr_pred_proba_val > 0.5).astype(int)\n",
    "\n",
    "lr_auc_val = roc_auc_score(y_val, lr_pred_proba_val)\n",
    "print(f\"Logistic Regression AUC (Validation): {lr_auc_val:.4f}\")\n",
    "print(\"\\nLogistic Regression Classification Report (Validation):\")\n",
    "print(classification_report(y_val, lr_pred_val, target_names=['Non-Diabétique', 'Diabétique']))\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, lr_pred_val))\n",
    "\n",
    "models_dict['Logistic Regression'] = {'model': lr_model, 'proba_val': lr_pred_proba_val, 'auc_val': lr_auc_val}\n",
    "# ## 7. Évaluation comparative et courbes ROC\n",
    "\n",
    "# Courbes ROC et Precision-Recall\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "for model_name, results in models_dict.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, results['proba_val'])\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC: {results['auc_val']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.7, label='Aléatoire')\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)')\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)')\n",
    "plt.title('Courbes ROC (Validation)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "for model_name, results in models_dict.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_val, results['proba_val'])\n",
    "    plt.plot(recall, precision, label=model_name)\n",
    "no_skill = len(y_val[y_val==1]) / len(y_val)\n",
    "plt.plot([0, 1], [no_skill, no_skill], 'k--', label='Pas de Compétence', alpha=0.7)\n",
    "plt.xlabel('Rappel (Recall)')\n",
    "plt.ylabel('Précision (Precision)')\n",
    "plt.title('Courbes Précision-Rappel (Validation)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "\n",
    "# Distribution des probabilités prédites\n",
    "plt.subplot(1, 3, 3)\n",
    "for model_name, results in models_dict.items():\n",
    "    sns.histplot(results['proba_val'][y_val == 0], bins=30, kde=True, stat=\"density\", label=f'{model_name} (Négatifs)', alpha=0.5)\n",
    "    sns.histplot(results['proba_val'][y_val == 1], bins=30, kde=True, stat=\"density\", label=f'{model_name} (Positifs)', alpha=0.5)\n",
    "plt.xlabel('Probabilité Prédite (Classe Positive)')\n",
    "plt.ylabel('Densité')\n",
    "plt.title('Distribution des Probabilités Prédites')\n",
    "plt.legend(fontsize='small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau de comparaison\n",
    "comparison_list = []\n",
    "for model_name, results in models_dict.items():\n",
    "    comparison_list.append({'Modèle': model_name, 'AUC_Validation': results['auc_val']})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_list)\n",
    "comparison_df = comparison_df.sort_values('AUC_Validation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== COMPARAISON DES MODÈLES (sur ensemble de validation) ===\")\n",
    "print(comparison_df.to_string(index=False))\n"
   ],
   "id": "5477466aa67b678e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ## 8. Génération des prédictions finales\n",
    "\n",
    "# Sélection du meilleur modèle basé sur AUC de validation\n",
    "if not comparison_df.empty:\n",
    "    best_model_name = comparison_df.iloc[0]['Modèle']\n",
    "    best_model_config = models_dict[best_model_name] # Contains the fitted model on X_train_final, y_train_final\n",
    "    print(f\"\\n=== MEILLEUR MODÈLE (basé sur AUC validation): {best_model_name} (AUC: {best_model_config['auc_val']:.4f}) ===\")\n",
    "\n",
    "    # Ré-entraîner le meilleur modèle sur l'ensemble des données d'entraînement sélectionnées (X_train_selected, y_train)\n",
    "    print(f\"Réentraînement du modèle {best_model_name} sur toutes les données d'entraînement ({X_train_selected.shape[0]} échantillons)...\")\n",
    "    \n",
    "    # Get the class and parameters of the best model\n",
    "    # This assumes the model objects in models_dict store their original parameters\n",
    "    # For sklearn models, get_params() is useful\n",
    "    final_model = best_model_config['model'].__class__(**best_model_config['model'].get_params())\n",
    "    \n",
    "    # Ensure parameters like class_weight or scale_pos_weight are correctly set for the full dataset if they depend on it\n",
    "    # For RandomForest and LogisticRegression with class_weight='balanced', it's handled internally.\n",
    "    # For XGBoost, scale_pos_weight might need recalculation if used:\n",
    "    # if best_model_name == 'XGBoost':\n",
    "    #     full_scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    #     final_model.set_params(scale_pos_weight=full_scale_pos_weight)\n",
    "\n",
    "    final_model.fit(X_train_selected, y_train) # Fit on all X_train_selected and y_train\n",
    "    \n",
    "    # Prédictions sur les données d'évaluation\n",
    "    final_predictions_proba = final_model.predict_proba(X_eval_selected)[:, 1]\n",
    "else:\n",
    "    print(\"Aucun modèle n'a été entraîné ou évalué. Impossible de générer les prédictions finales.\")\n",
    "    final_predictions_proba = np.zeros(len(sample_submission)) # Fallback\n",
    "\n",
    "# Création du fichier de soumission\n",
    "submission_df = sample_submission.copy()\n",
    "submission_df['target'] = final_predictions_proba\n",
    "\n",
    "# Sauvegarde\n",
    "submission_filename = './submission_t2d_prediction_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"\\nPrédictions sauvegardées dans: {submission_filename}\")\n",
    "print(f\"Statistiques des prédictions finales (probabilités):\")\n",
    "print(f\"  - Min: {final_predictions_proba.min():.4f}\")\n",
    "print(f\"  - Max: {final_predictions_proba.max():.4f}\")\n",
    "print(f\"  - Moyenne: {final_predictions_proba.mean():.4f}\")\n",
    "print(f\"  - Médiane: {np.median(final_predictions_proba):.4f}\")\n",
    "print(f\"  - Std Dev: {np.std(final_predictions_proba):.4f}\")\n",
    "\n",
    "# Distribution des prédictions finales\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(final_predictions_proba, bins=50, kde=True, color='skyblue')\n",
    "plt.xlabel('Probabilité Prédite (sur données d\\'évaluation)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title(f'Distribution des Prédictions Finales - {best_model_name if not comparison_df.empty else \"N/A\"}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(np.mean(final_predictions_proba), color='red', linestyle='--', label=f'Moyenne: {np.mean(final_predictions_proba):.3f}')\n",
    "plt.axvline(np.median(final_predictions_proba), color='green', linestyle=':', label=f'Médiane: {np.median(final_predictions_proba):.3f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPourcentage prédit comme positif avec seuil 0.5 (sur données d'évaluation): {np.mean(final_predictions_proba > 0.5)*100:.2f}%\")\n",
    "\n",
    "\n",
    "# ## 9. Résumé et recommandations\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RÉSUMÉ DE L'ANALYSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📊 Dataset: {X_train.shape[0]} patients × {X_train.shape[1]} mois × {X_train.shape[2]} features originales ({len(feature_names_original)} noms)\")\n",
    "print(f\"🎯 Taux de cas positifs (entraînement): {np.mean(y_train)*100:.2f}%\")\n",
    "print(f\"🔩 Features ingéniérées: {X_train_engineered.shape[1]} features ({len(engineered_feature_names)} noms)\")\n",
    "print(f\"✅ Features sélectionnées pour modélisation: {X_train_selected.shape[1]}\")\n",
    "if not comparison_df.empty:\n",
    "    print(f\"🏆 Meilleur modèle (sur validation): {best_model_name} (AUC validation: {best_model_config['auc_val']:.4f})\")\n",
    "else:\n",
    "    print(\"🏆 Meilleur modèle: Non déterminé.\")\n",
    "print(f\"📈 Prédictions moyennes sur l'ensemble d'évaluation: {np.mean(final_predictions_proba):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PISTES D'AMÉLIORATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. 🔬 Feature Engineering Approfondi:\")\n",
    "print(\"   - Interactions entre features (ex: ratio glucose/insuline si disponibles).\")\n",
    "print(\"   - Features basées sur des seuils cliniques (ex: nombre de fois où la glycémie > X).\")\n",
    "print(\"   - Utilisation de transformées (Fourier, ondelettes) pour capturer des périodicités.\")\n",
    "print(\"   - Features de type 'time since last abnormal value'.\")\n",
    "print(\"   - Traitement plus fin des valeurs manquantes (imputation multiple, modélisation des NaN).\")\n",
    "\n",
    "print(\"\\n2. 🧠 Modélisation Avancée:\")\n",
    "print(\"   - Modèles d'ensemble plus sophistiqués (Stacking, Blending).\")\n",
    "print(\"   - Réseaux de neurones adaptés aux séquences (LSTM, GRU, Transformers) appliqués directement sur les données X_train (non-engineered).\")\n",
    "print(\"   - Modèles tenant compte de la structure temporelle de manière explicite lors de la validation croisée (ex: TimeSeriesSplit plus rigoureux).\")\n",
    "\n",
    "print(\"\\n3. ⚙️ Optimisation et Calibration:\")\n",
    "print(\"   - Optimisation fine des hyperparamètres (ex: Optuna, Hyperopt, GridSearchCV avec K-Fold temporel).\")\n",
    "print(\"   - Optimisation du seuil de décision sur l'ensemble de validation pour maximiser une métrique métier (ex: F1-score, sensibilité à un rappel donné).\")\n",
    "print(\"   - Calibration des probabilités (ex: Isotonic Regression, Platt Scaling) pour que les scores de probabilité soient plus fiables.\")\n",
    "\n",
    "print(\"\\n4. 📖 Interprétabilité et Analyse d'Erreurs:\")\n",
    "print(\"   - Utilisation de SHAP ou LIME pour comprendre les prédictions au niveau individuel et global.\")\n",
    "print(\"   - Analyse détaillée des faux positifs et faux négatifs: y a-t-il des profils de patients pour lesquels le modèle se trompe systématiquement?\")\n",
    "print(\"   - Évaluation de la robustesse du modèle face à de légères variations des données d'entrée.\")\n",
    "\n",
    "print(\"\\n5. 💾 Gestion des Données et Pipeline:\")\n",
    "print(\"   - Mettre en place un pipeline ML robuste (ex: Scikit-learn Pipeline, MLflow) pour la reproductibilité et le déploiement.\")\n",
    "print(\"   - Versionnement des données et des modèles.\")\n",
    "\n",
    "print(f\"\\n✅ Analyse terminée! Fichier de soumission généré: {submission_filename}\")"
   ],
   "id": "79294f997bd285d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6eeab88bac68b7ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
